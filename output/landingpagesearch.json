[{"categories":null,"content":"Benchmark Report Test purpose This test aims at comparing the OLAP benchmark performance of various data lake formats in the scenario of continuous streaming ingestion in the CDC database.\nMeanwhile, particular attention was paid during the testing process to the impact of enabling self-optimizing on the analytical performance of the table.\nTest envrionment Hardware configuration Number OS Cpu core Memory Disk type Deployed components 1 CentOS 7 40 256 SAS HDFS、Hive、Yarn 2 CentOS 7 40 256 SAS HDFS、Hive、Yarn 3 CentOS 7 40 256 SAS HDFS、Hive、Yarn 4 CentOS 7 40 256 SAS Trino、Presto 5 CentOS 7 40 256 SAS Trino、Presto 6 CentOS 7 40 256 SAS Trino、Presto Software version Software Version Trino 380 Presto 274 Iceberg 0.13 Amoro 0.4 Hudi 0.11.1 Test plan Overview This test is based on CHbenchmark, which is a hybrid testing standard that integrates TPCC and TPCH. The overall testing workload can be divided into two categories:\n5 OLTP workloads based on TPC-C: NewOrder, Payment, OrderStatus, Delivery, and StockLevel.\n22 OLAP workloads based on TCP-H, where Q15 was abandoned in this test due to its association with views and the lack of view rewriting.\nPrepare test data Based on the TPC-C, the raw data was constructed in MySQL for this test. The dataset includes a total of 12 tables, with the relationship between TPC-C and TPC-H tables shown in the following diagram:\nIn addition, the relationship between the data sizes of each table is shown in the following table, where w represents the number of warehouses. It can be observed that the data sizes of intermediate tables such as new_order and stock are affected by the number of warehouses. Therefore, the data set size can be adjusted by controlling the number of warehouses during testing.\nIn this test, the number of warehouses was set to 100, and the initial data set size in the MySQL database was approximately 10GB. The following table shows the number of data records for each table in the initial data set and the changes in the number of data records for each table after running the one-hour TPC-C test.\nTable name The number of records (rows) in beginning The number of records (rows) after running a one-hour TPC-C test warehouse 100 100 item 100000 100000 stock 10000000 10000000 district 1000 1000 customer 3000000 3000000 history 3000000 3119285（+119285） oorder 3000000 3124142（+124142） new_order 893709 907373（+13664） order_line 29996774 31252799（+1256025） region 5 5 nation 62 62 supplier 1000 1000 Perform the test Before starting TPC-H testing, start a Flink job to synchronize both the history data and real-time data from MySQL into the data lake. Afterwards, execute TPC-H testing through query engines such as Trino or Presto.\nTPC-H contains 22 query statements, only three of which are listed here due to space limitation:\n-- query1 SELECT ol_number, sum(ol_quantity) AS sum_qty, sum(ol_amount) AS sum_amount, avg(ol_quantity) AS avg_qty, avg(ol_amount) AS avg_amount, count(*) AS count_order FROM order_line WHERE ol_delivery_d \u003e '2007-01-02 00:00:00.000000' GROUP BY ol_number ORDER BY ol_number; -- query2 SELECT su_suppkey, su_name, n_name, i_id, i_name, su_address, su_phone, su_comment FROM item, supplier, stock, nation, region, (SELECT s_i_id AS m_i_id, MIN(s_quantity) AS m_s_quantity FROM stock, supplier, nation, region WHERE MOD((s_w_id*s_i_id), 10000) = su_suppkey AND su_nationkey = n_nationkey AND n_regionkey = r_regionkey AND r_name LIKE 'Europ%' GROUP BY s_i_id) m WHERE i_id = s_i_id AND MOD((s_w_id * s_i_id), 10000) = su_suppkey AND su_nationkey = n_nationkey AND n_regionkey = r_regionkey AND i_data LIKE '%b' AND r_name LIKE 'Europ%' AND i_id=m_i_id AND s_quantity = m_s_quantity ORDER BY n_name, su_name, i_id; -- query3 SELECT ol_o_id, ol_w_id, ol_d_id, sum(ol_amount) AS revenue, o_entry_d FROM customer, new_order, oorder, order_line WHERE c_state LIKE 'A%' AND c_id = o_c_id AND c_w_id = o_w_id AND c_d_id = o_d_id AND no_w_id = o_w_id AND no_d_id = o_d_id AND no_o_id = o_id AND ol_w_id = o_w_id AND ol_d_id = o_d_id AND ol_o_id = o_id AND o_entry_d \u003e '2007-01-02 00:00:00.000000' GROUP BY ol_o_id, ol_w_id, ol_d_id, o_entry_d ORDER BY revenue DESC, o_entry_d; Test results Static result The figure above shows a performance comparison of Iceberg and Mixed-Iceberg table formats for querying static data. It can be seen from the figure that the query performance of the two table formats is similar.\nDynamic result The figure above shows a performance comparison of Iceberg 、Mixed-Iceberg and Hudi table formats for querying dynamic data. The test recorded the results of running TPC-C for different time periods.\nThe following are the specific results of each test group:\nConclusion In the case of static data, the query performance of Iceberg and Mixed-Iceberg tables is similar. Without enabling self-optimizing, the query performance of all table formats will continue to decline as dynamic data is continuously written. After enabling self-optimizing, the query performance of all table formats remains stable as dynamic data is continuously written. ","description":"","title":"Benchmark Report","uri":"https://amoro.apache.org/benchmark-report/"},{"categories":null,"content":"Download Please choose an Amoro version to download from the following tables. It is recommended you use the latest release.\nThe latest release Coming soon!\nNon-Apache releases These releases were made before the Amoro project joined the ASF Incubator and have not followed the usual ASF release process.\nVersion Date Source AMS Flink Runtime Jars Spark Runtime Jars Trino Connector Release Notes 0.6.1 (non-Apache) 2024 Feb 21 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.15 Runtime Jar - Flink 1.16 Runtime Jar - Flink 1.17 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note Version Date Source AMS Flink Runtime Jars Spark Runtime Jars Trino Connector Release Notes 0.6.0 2023 Nov 6 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.15 Runtime Jar - Flink 1.16 Runtime Jar - Flink 1.17 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.5.1 2023 Oct 10 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.5.0 2023 Aug 8 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.4.1 2023 Apr 3 source AMS - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 2.3 Runtime Jar\n- Spark 3.1 Runtime Jar Trino Connector release note 0.4.0 2022 Dec 6 source AMS - Flink 1.12 Runtime Jar\n- Flink 1.14 Runtime Jar\n- Flink 1.15 Runtime Jar - Spark 2.3 Runtime Jar\n- Spark 3.1 Runtime Jar Trino Connector release note ","description":"","title":"Download","uri":"https://amoro.apache.org/download/"},{"categories":null,"content":"Benchmark Guild This guilde introduces detailed steps for executing the benchmark to validate performance of various data lake formats.\nBy following the steps in the guild, you can learn about the analytical performance of different data lake table format. At the same time, you can flexibly adjust the test scenarios to obtain test results that better suit your actual scenario.\nDeploy testing environment Deploy by Docker With Docker-Compose, you can quickly set up an environment for performing the benchmark. The detailed steps reference: Lakehouse-benchmark.\nDeploy manually Alternatively, you can manually deploy the following components to set up the test environment：\nComponent Version Description Installation Guide MySQL 5.7+ MySQL is used to generate TPC-C data for synchronization to data lakes. MySQL Installation Guide Hadoop 2.3.7+ Hadoop is used to provide the storage for data lakes. Ambari Trino 380 Trino is used to execute TPC-H queries for Iceberg and Mixed-Iceberg format tables. Trino Installation Guide Amoro Trino Connector 0.4.0 To query Mixed-Iceberg Format tables in Trino, you need to install and configure the Amoro connector in Trino. Amoro Trino Connector Iceberg Trino Connector 0.13.0 To query Iceberg Format tables in Trino, you need to install and configure the Iceberg connector in Trino. Iceberg Trino Connector Presto 274 Presto is used to execute TPC-H queries for Hudi format tables. Presto Installation Guide Hudi Presto Connector 0.11.1 To query Iceberg Format tables in Trino, you need to install and configure the Iceberg connector in Presto. Hudi Presto Connector AMS 0.4.0 Amoro Management Service, support self-optimizing on tables during the test. AMS Installation Guide data-lake-benchmark 21 The core program of Benchmark which is responsible for generating test data, executing the testing process, and generating test results. Data Lake Benchmark lakehouse-benchmark-ingestion 1.0 Data synchronization tool based on Flink-CDC which can synchronize data from database to data lake in real-time. Lakehouse Benchmark Ingestion Benchmark steps Configure the configuration file config/mysql/sample_chbenchmark_config.xml file of program data-lake-benchmark. Fill in the information of MySQL and parameter scalefactor. scalefactor represents the number of warehouses, which controls the overall data volume. Generally, choose 10 or 100.\nGenerate static data into MySQL with command：\njava -jar lakehouse-benchmark.jar -b tpcc,chbenchmark -c config/mysql/sample_chbenchmark_config.xml --create=true --load=true Configure the configuration file config/ingestion-conf.yaml file of program lakehouse-benchmark-ingestion. Fill in the information of MySQL.\nStart the ingestion job to synchronize data form MySQL to data lake tables witch command:\njava -cp lakehouse-benchmark-ingestion-1.0-SNAPSHOT.jar com.netease.arctic.benchmark.ingestion.MainRunner -confDir [confDir] -sinkType [arctic/iceberg/hudi] -sinkDatabase [dbName] Execute TPC-H benchmark on static data with command: java -jar lakehouse-benchmark.jar -b chbenchmarkForTrino -c config/trino/trino_chbenchmark_config.xml --create=false --load=false --execute=true Execute TPC-C program to continuously write data into MYSQL witch command: java -jar lakehouse-benchmark.jar -b tpcc,chbenchmark -c config/mysql/sample_chbenchmark_config.xml --execute=true -s 5 Execute TPC-H benchmark on dynamic data with command: java -jar lakehouse-benchmark.jar -b chbenchmarkForTrino -c config/trino/trino_chbenchmark_config.xml --create=false --load=false --execute=true Obtain the benchmark results in the result directory of the data-lake-benchmark project.\nRepeat step 7 to obtain benchmark results for different points in time.\n","description":"","title":"How To Benchmark","uri":"https://amoro.apache.org/benchmark-guide/"},{"categories":null,"content":"Contributing to Amoro Thanks for your interest in the Amoro project. Contributions are welcome and are greatly appreciated! Every little effort helps, and credit will always be given.\nThis page provides some orientation and resources for getting involved with the project. It also offers recommendations for the best results when engaging with the community. We hope this will be a pleasant first experience for you and you’ll return to continue contributing.\nGet Involved If you have any questions, suggestions, or improvement ideas when using Amoro, you can participate in the Amoro community building through the following suggested channels.\nIssue Tracker - for tracking bugs, ideas, plans, etc. GitHub Discussions - second to the mailing list for anything else you want to share or ask. WeChat Group - add kllnn999 as a friend on WeChat and specify Amoro lover. You will be invited to join the Amoro WeChat group. Contributing Guide See Contributing for more details on contributing to Amoro.\n","description":"","title":"How To Contribute","uri":"https://amoro.apache.org/how-to-contribute/"},{"categories":null,"content":"Welcome to Amoro Amoro community is a free, open-source community project. Anyone interested in the Amoro project can join the community and contribute to its development by becoming a part of it.\nThis document describes the various roles within the community, including the responsibilities of the roles and the criteria for contribution.\nRoles and Responsibilities Amoro community is composed of and operated by the following roles:\nUser Contributor Active Contributor Committer PMC User Community users, as defined by Amoro, are those members of the community who need the Amoro project, either individuals or businesses.\nContributor Everyone who contributes can become an Amoro contributor. The members will provide mentorship and guidance when new contributors need assistance.\nHow to become a Contributor? 1 merged PR in this project Responsibilities and privileges Actively participate in Amoro’s project development Participate in community events (meetups, hackathons, etc.) Learn and help others learn Amoro-related technologies Be listed as an Amoro contributor Be awarded an Amoro Contributor e-certificate Active Contributor Active contributors have made outstanding contributions and sustained commitment to Amoro. They actively participate in the community by contributing code, improving documents, and helping others.\nHow to become an Active Contributor? Have 2 merged PRs or fixed major bugs Consistently active in the community and actively participate in community events such as online/offline meetups and community discussions Responsibilities and privileges Join the community meeting and discussion Mentor and guide new contributors Be listed as an Amoro Active Contributor and featured on the Amoro official website Be awarded an Amoro Active Contributor e-certificate Committer Committers are promoted from Active Contributors. They have the authority to merge master branches and are responsible for the planning and maintenance of Amoro. They are also active members who share their knowledge with the community.\nHow to become a Committer? Have a deep understanding of Amoro’s principles and future plans Have the ability to deal with various issues that arise in the project promptly Lead a major development, write and revise related documents Be voted in by the Amoro PPMC Responsibilities and privileges Mentor and guide other memberships in the community Ensure continued health of subproject Be granted write access to Amoro repositories Be listed as an Amoro Committer and featured on the Amoro official website Be awarded an Amoro Committer e-certificate PPMC Members PPMC members are responsible for the planning and maintenance of Amoro. They are also active members who share their knowledge with the community.\nHow to become a PPMC member? Have the ability to deal with project issues Lead project development and iterations, and steer the overall direction of the project Be voted in by the Amoro PPMC Responsibilities and privileges Mentor and guide other memberships in the community Ensure continued health of the project, such as code quality and test coverage Make and approve technical design decisions Define milestones and releases Vote and promote new committers and PPMC members Be listed as an Amoro PPMC member and featured on the Amoro official website Be awarded an Amoro PPMC e-certificate ","description":"","title":"Join Community","uri":"https://amoro.apache.org/join-community/"},{"categories":null,"content":"Quick Demo This guide outlines the basic process of using Amoro, allowing you to quickly experience its core features. You can choose to use either the Iceberg Format or the Mixed-Iceberg Format to complete the entire process.\nIf you are more interested in the Mixed-Hive Format or the Paimon Format, you can refer to: Mixed-Hive Format and Paimon Format. For specific information on the different formats supported by Amoro, please refer to: Table Format.\nBefore starting the quick demo, some steps are required to prepare the environment. Here are two ways to complete the necessary preparation:\nSetup from Docker-Compose Setup from binary release Prepare steps Create optimizer group Open http://localhost:1630 in a browser, enter admin/admin to log in to the dashboard.\nClick on Optimizing in the sidebar, choose Optimizer Groups and click Add Group button to create a new group befre creating catalog:\nCreate catalog Click on Catalogs in the sidebar, click on the + button under Catalog List to create a test catalog, and name it to demo_catalog:\nIceberg Format Mixed-Iceberg Format To use the Iceberg Format, select Type as Internal Catalog, and choose Iceberg as Table Format. To use the Mixed-Iceberg Format, select Type as Internal Catalog, and choose Mixed-Iceberg as Table Format. If you deployed the demo environment using Docker-Compose:\nHadoop configuration files are stored in \u003cARCIT-WORKSPACE\u003e/hadoop-config. Warehouse path should be set to hdfs://namenode:8020/user/arctic/demo_warehouse. If you deployed the demo environment through binary release:\nDon’t need to upload the Hadoop configuration files. Warehouse path should be set a directory which the hadoop user have the read and write privileges. Start optimizers Click on Optimizing in the sidebar, select the Optimizer Group tab, and click the scale-out operation for group local.\nSet the concurrency of the optimizer to 1 and click OK.\nThen you can switch the tab to Optimizers, you can find the newly launched optimizer here.\nYou may need to wait for up to 30 seconds for the optimizer to register with AMS. Demo setps Initialize tables Click on Terminal in the sidebar, you can create the test tables here using SQL. Terminal supports executing Spark SQL statements for now.\nIceberg Format Mixed-Iceberg Format CREATE DATABASE IF NOT EXISTS db; USE db; CREATE TABLE IF NOT EXISTS user ( id INT, name string, ts TIMESTAMP ) USING iceberg PARTITIONED BY (days(ts)); INSERT OVERWRITE user VALUES (1, \"eric\", timestamp(\"2022-07-01 12:32:00\")), (2, \"frank\", timestamp(\"2022-07-02 09:11:00\")), (3, \"lee\", timestamp(\"2022-07-02 10:11:00\")); SELECT * FROM user; CREATE DATABASE IF NOT EXISTS db; USE db; CREATE TABLE IF NOT EXISTS user ( id INT, name string, ts TIMESTAMP, PRIMARY KEY(id) ) USING arctic PARTITIONED BY (days(ts)); INSERT OVERWRITE user VALUES (1, \"eric\", timestamp(\"2022-07-01 12:32:00\")), (2, \"frank\", timestamp(\"2022-07-02 09:11:00\")), (3, \"lee\", timestamp(\"2022-07-02 10:11:00\")); SELECT * FROM user; Click on the RUN button uppon the SQL editor, and wait for the SQL query to finish executing. You can then see the query results under the SQL editor.\nStart Flink ingestion job If you have prepared the environment using Docker-Compose, you can open a terminal in docker using the following command:\ndocker exec -it quickdemo bash Then you can start the standalone Flink cluster using the following command:\ncd \u003cFLINK_DIR\u003e ./bin/start-cluster.sh After you start the Flink cluster, you can open the Flink dashboard by visiting http://localhost:8081.\nThen execute the following command to start the Flink SQL Client:\n./bin/sql-client.sh embedded Enter the following SQL statements one by one to start the Flink ingestion job since the Flink SQL Client does not support batch SQL input:\nIceberg Format Mixed-Iceberg Format CREATE CATALOG iceberg_catalog WITH ( 'type' = 'iceberg', 'catalog-impl' = 'org.apache.iceberg.rest.RESTCatalog', 'uri'='http://127.0.0.1:1630/api/iceberg/rest', 'warehouse'='demo_catalog' ); -- Recreate table with Flink as Only Flink support primary key for iceberg format table DROP TABLE `iceberg_catalog`.`db`.`user`; CREATE TABLE IF NOT EXISTS `iceberg_catalog`.`db`.`user` ( id INT, name string, ts TIMESTAMP_LTZ, PRIMARY KEY(id) NOT ENFORCED ) with ('format-version'='2', 'write.upsert.enabled'='true'); INSERT INTO iceberg_catalog.`db`.`user` (id, name, ts) VALUES (1, 'eric', CAST('2022-07-01 12:32:00' AS TIMESTAMP)), (2, 'frank', CAST('2022-07-02 09:11:00' AS TIMESTAMP)), (3, 'lee', CAST('2022-07-02 10:11:00' AS TIMESTAMP)); -- Create CDC socket source table CREATE TABLE cdc_source( id INT, name STRING, op_time STRING ) WITH ( 'connector' = 'socket', 'hostname' = 'localhost', 'port' = '9999', 'format' = 'changelog-csv', 'changelog-csv.column-delimiter' = '|' ); -- Disable Flink engine form filtering on Delete data set table.exec.sink.upsert-materialize=none; -- Start the Flink ingestion job INSERT INTO `iceberg_catalog`.`db`.`user` SELECT id, name, CAST(TO_TIMESTAMP(op_time) AS TIMESTAMP(6) WITH LOCAL TIME ZONE) ts FROM cdc_source; CREATE CATALOG arctic_catalog WITH ( 'type' = 'arctic', 'metastore.url'='thrift://127.0.0.1:1260/demo_catalog' ); -- Create CDC socket source table CREATE TABLE cdc_source( id INT, name STRING, op_time STRING ) WITH ( 'connector' = 'socket', 'hostname' = 'localhost', 'port' = '9999', 'format' = 'changelog-csv', 'changelog-csv.column-delimiter' = '|' ); -- Disable Flink engine form filtering on Delete data set table.exec.sink.upsert-materialize=none; -- Start the Flink ingestion job INSERT INTO `arctic_catalog`.`db`.`user` SELECT id, name, CAST(TO_TIMESTAMP(op_time) AS TIMESTAMP(6) WITH LOCAL TIME ZONE) ts FROM cdc_source; Open a new terminal. If you deployed the demo environment using Docker-Compose, you can use the following command to reopen a terminal of the container. The required commands are already installed inside the container.\ndocker exec -it quickdemo bash Execute the following command in the newly opened terminal. It will open a socket channel to the cdc_source table, allowing us to insert some test data into the table.\nnc -l -p 9999 -k if you are in MacOS and run quick demo in local. run nc -lk 9999 instead. Send the following content into the socket channel, and make sure to add a break line after the last line of data to ensure that the last line of data is correctly sent.\nINSERT|4|rock|2022-07-02 09:01:00 INSERT|5|jack|2022-07-02 12:11:40 INSERT|6|mars|2022-07-02 11:19:10 Wait for at least 10 seconds (depending on the checkpoint interval configured in flink-conf.yaml), open the Dashboard and go to the Terminal page, then execute:\nSELECT * FROM db.user ORDER BY id; You will get the following execution result:\nContinue to send the following data into the socket channel:\nDELETE|1|eric|2022-07-01 12:32:00 INSERT|7|randy|2022-07-03 19:11:00 DELETE|4|rock|2022-07-02 09:01:00 DELETE|3|lee|2022-07-02 10:11:00 Query the content of the test table by Terminal again, you will get the following result this time：\nCheck self-optimizing As new data is written to the table, Amoro will automatically trigger self-optimizing on the table.\nClick on Tables in the sidebar, select the test table to enter the table details page, switch to the Optimizing tab, where you can see all the self-optimizing tasks on the table.\nYou can also enter the Optimizing page through the the sidebar to view the current optimizing status of all tables.\nFor more information on Self-Optimizing, please refer to: Self-optimizing\n","description":"","title":"Quick Demo","uri":"https://amoro.apache.org/quick-demo/"},{"categories":null,"content":"Setup This guide describes two ways to deploy the Amoro demo environment: using docke-compose or release packages. If you want to deploy by compiling the source code, please refer to Deployment.\nSetup from Docker-Compose The fastest way to deploy a Quick Demo environment is to use docker-compose.\nRequirements Before starting to deploy Amoro based on Docker, please make sure that you have installed the docker-compose environment on your host. For information on how to install Docker, please refer to: Install Docker.\nIt is recommended to perform the operation on Linux or MacOS. If you are using a Windows system, you can consider using WSL2. For information on how to enable WSL2 and install Docker, please refer to Windows Installation. After completing the Docker installation, please make sure that the docker-compose tool is installed: Docker-Compose Installation.\nBring up demo cluster Before starting, please prepare a clean directory as the workspace for Amoro Demo deployment, and obtain the Amoro demo deployment script:\ncd \u003cAMORO-WORKSPACE\u003e wget https://raw.githubusercontent.com/apache/incubator-amoro/master/docker/demo-cluster.sh Execute the following shell command to launch a demo cluster using docker-compose:\nbash demo-cluster.sh start After executing the above command, there will be a data directory in the workspace directory for sharing files between different docker containers. You can use the following command to view all the running Docker containers:\n$ docker ps --format \"table {{.ID}}\\t{{.Names}}\\t{{.Status}}\" CONTAINER ID NAMES STATUS 2a0d326c668f datanode Up 1 minutes 8e6b1c36e6ba namenode Up 1 minutes 1e11fb8187b3 quickdemo Up 1 minutes Setup from binary release If it is not convenient to install Docker and related tools, you can also deploy the Amoro demo cluster directly through the Amoro release package.\nRequirements Before starting, please make sure that Java 8 is installed and the JAVA_HOME environment variable is set. Please make sure that there is no HADOOP_HOME or HADOOP_CONF_DIR in the environment variables. If there are, please unset these environment variables first.\nSetup AMS Prepare a clean directory as the workspace for the Amoro demo cluster, and execute the following command to download Amoro and start AMS:\ncd \u003cAMORO-WORKSPACE\u003e # Rplace version value with the latest Amoro version if needed export AMORO_VERSION=0.6.0 # Download the binary package of AMS wget https://github.com/apache/incubator-amoro/releases/download/v${AMORO_VERSION}/amoro-${AMORO_VERSION}-bin.zip # Unzip the pakage unzip amoro-${AMORO-VERSION}-bin.zip # Start AMS by script cd amoro-${AMORO-VERSION} \u0026\u0026 ./bin/ams.sh start Access http://127.0.0.1:1630/ with a browser and log in to the system with admin/admin. If you can log in successfully, it means that the deployment of AMS is successful.\nSetup Flink environment Before starting the Quick Demo, you also need to deploy the Flink execution environment. Execute the following command to download the Flink binary distribution package:\ncd \u003cAMORO-WORKSPACE\u003e # Rplace version value with the latest Amoro version if needed AMORO_VERSION=0.6.0 ICEBERG_VERSION=1.3.0 FLINK_VERSION=1.15.3 FLINK_MAJOR_VERSION=1.15 FLINK_HADOOP_SHADE_VERSION=2.7.5 APACHE_FLINK_URL=archive.apache.org/dist/flink MAVEN_URL=https://repo1.maven.org/maven2 FLINK_CONNECTOR_URL=${MAVEN_URL}/org/apache/flink AMORO_CONNECTOR_URL=${MAVEN_URL}/com/apache/incubator-amoro ICEBERG_CONNECTOR_URL=${MAVEN_URL}/org/apache/iceberg # Download FLink binary package wget ${APACHE_FLINK_URL}/flink-${FLINK_VERSION}/flink-${FLINK_VERSION}-bin-scala_2.12.tgz # Unzip Flink binary package tar -zxvf flink-${FLINK_VERSION}-bin-scala_2.12.tgz cd flink-${FLINK_VERSION} # Download Flink Hadoop dependency wget ${FLINK_CONNECTOR_URL}/flink-shaded-hadoop-2-uber/${FLINK_HADOOP_SHADE_VERSION}-10.0/flink-shaded-hadoop-2-uber-${FLINK_HADOOP_SHADE_VERSION}-10.0.jar # Download Flink Aoro Connector wget ${AMORO_CONNECTOR_URL}/amoro-flink-runtime-${FLINK_MAJOR_VERSION}/${AMORO_VERSION}/amoro-flink-runtime-${FLINK_MAJOR_VERSION}-${AMORO_VERSION}.jar # Download Flink Iceberg Connector wget ${ICEBERG_CONNECTOR_URL}/iceberg-flink-runtime-${FLINK_MAJOR_VERSION}/${ICEBERG_VERSION}/iceberg-flink-runtime-${FLINK_MAJOR_VERSION}-${ICEBERG_VERSION}.jar # Copy the necessary JAR files to the lib directory mv flink-shaded-hadoop-2-uber-${FLINK_HADOOP_SHADE_VERSION}-10.0.jar lib mv amoro-flink-runtime-${FLINK_MAJOR_VERSION}-${AMORO_VERSION}.jar lib mv iceberg-flink-runtime-${FLINK_MAJOR_VERSION}-${ICEBERG_VERSION}.jar lib cp examples/table/ChangelogSocketExample.jar lib Finally, we need to make some modifications to the flink-conf.yaml configuration file.\nvim conf/flink-conf.yaml # Increase the number of slots to run more streaming tasks taskmanager.numberOfTaskSlots: 4 # Enable checkpointing and to see data changes more quickly, set the checkpoint interval to 5 seconds. execution.checkpointing.interval: 5s ","description":"","title":"Quickstart Setup","uri":"https://amoro.apache.org/quickstart-setup/"},{"categories":null,"content":"Roadmap This roadmap displays the workings that the Amoro community plans to complete in 2024 and their priorities. Each item is associated with a Github issue, where you can learn about its specific design and latest developments.\nPriority 1 Refactor AMS using function and process api Add metric collection for ams Support table sorting process Serverless process: automatic scaling for process resources Monitor the health status of tables Improve Mixed format Priority 2 Support Apache Hudi Support ingestion process Maintain secondary indexes on tables Collect scan metrics and improve optimizing rules based on them Enhance the management capabilities of the Paimon format ","description":"","title":"Roadmap","uri":"https://amoro.apache.org/roadmap/"}]