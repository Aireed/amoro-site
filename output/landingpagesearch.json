[{"categories":null,"content":"Benchmark Report Test purpose This test aims at comparing the OLAP benchmark performance of various data lake formats in the scenario of continuous streaming ingestion in the CDC database.\nMeanwhile, particular attention was paid during the testing process to the impact of enabling self-optimizing on the analytical performance of the table.\nTest envrionment Hardware configuration Number OS Cpu core Memory Disk type Deployed components 1 CentOS 7 40 256 SAS HDFS、Hive、Yarn 2 CentOS 7 40 256 SAS HDFS、Hive、Yarn 3 CentOS 7 40 256 SAS HDFS、Hive、Yarn 4 CentOS 7 40 256 SAS Trino、Presto 5 CentOS 7 40 256 SAS Trino、Presto 6 CentOS 7 40 256 SAS Trino、Presto Software version Software Version Trino 380 Presto 274 Iceberg 0.13 Amoro 0.4 Hudi 0.11.1 Test plan Overview This test is based on CHbenchmark, which is a hybrid testing standard that integrates TPCC and TPCH. The overall testing workload can be divided into two categories:\n5 OLTP workloads based on TPC-C: NewOrder, Payment, OrderStatus, Delivery, and StockLevel.\n22 OLAP workloads based on TCP-H, where Q15 was abandoned in this test due to its association with views and the lack of view rewriting.\nPrepare test data Based on the TPC-C, the raw data was constructed in MySQL for this test. The dataset includes a total of 12 tables, with the relationship between TPC-C and TPC-H tables shown in the following diagram:\nIn addition, the relationship between the data sizes of each table is shown in the following table, where w represents the number of warehouses. It can be observed that the data sizes of intermediate tables such as new_order and stock are affected by the number of warehouses. Therefore, the data set size can be adjusted by controlling the number of warehouses during testing.\nIn this test, the number of warehouses was set to 100, and the initial data set size in the MySQL database was approximately 10GB. The following table shows the number of data records for each table in the initial data set and the changes in the number of data records for each table after running the one-hour TPC-C test.\nTable name The number of records (rows) in beginning The number of records (rows) after running a one-hour TPC-C test warehouse 100 100 item 100000 100000 stock 10000000 10000000 district 1000 1000 customer 3000000 3000000 history 3000000 3119285（+119285） oorder 3000000 3124142（+124142） new_order 893709 907373（+13664） order_line 29996774 31252799（+1256025） region 5 5 nation 62 62 supplier 1000 1000 Perform the test Before starting TPC-H testing, start a Flink job to synchronize both the history data and real-time data from MySQL into the data lake. Afterwards, execute TPC-H testing through query engines such as Trino or Presto.\nTPC-H contains 22 query statements, only three of which are listed here due to space limitation:\n-- query1 SELECT ol_number, sum(ol_quantity) AS sum_qty, sum(ol_amount) AS sum_amount, avg(ol_quantity) AS avg_qty, avg(ol_amount) AS avg_amount, count(*) AS count_order FROM order_line WHERE ol_delivery_d \u003e '2007-01-02 00:00:00.000000' GROUP BY ol_number ORDER BY ol_number; -- query2 SELECT su_suppkey, su_name, n_name, i_id, i_name, su_address, su_phone, su_comment FROM item, supplier, stock, nation, region, (SELECT s_i_id AS m_i_id, MIN(s_quantity) AS m_s_quantity FROM stock, supplier, nation, region WHERE MOD((s_w_id*s_i_id), 10000) = su_suppkey AND su_nationkey = n_nationkey AND n_regionkey = r_regionkey AND r_name LIKE 'Europ%' GROUP BY s_i_id) m WHERE i_id = s_i_id AND MOD((s_w_id * s_i_id), 10000) = su_suppkey AND su_nationkey = n_nationkey AND n_regionkey = r_regionkey AND i_data LIKE '%b' AND r_name LIKE 'Europ%' AND i_id=m_i_id AND s_quantity = m_s_quantity ORDER BY n_name, su_name, i_id; -- query3 SELECT ol_o_id, ol_w_id, ol_d_id, sum(ol_amount) AS revenue, o_entry_d FROM customer, new_order, oorder, order_line WHERE c_state LIKE 'A%' AND c_id = o_c_id AND c_w_id = o_w_id AND c_d_id = o_d_id AND no_w_id = o_w_id AND no_d_id = o_d_id AND no_o_id = o_id AND ol_w_id = o_w_id AND ol_d_id = o_d_id AND ol_o_id = o_id AND o_entry_d \u003e '2007-01-02 00:00:00.000000' GROUP BY ol_o_id, ol_w_id, ol_d_id, o_entry_d ORDER BY revenue DESC, o_entry_d; Test results Static result The figure above shows a performance comparison of Iceberg and Mixed-Iceberg table formats for querying static data. It can be seen from the figure that the query performance of the two table formats is similar.\nDynamic result The figure above shows a performance comparison of Iceberg 、Mixed-Iceberg and Hudi table formats for querying dynamic data. The test recorded the results of running TPC-C for different time periods.\nThe following are the specific results of each test group:\nConclusion In the case of static data, the query performance of Iceberg and Mixed-Iceberg tables is similar. Without enabling self-optimizing, the query performance of all table formats will continue to decline as dynamic data is continuously written. After enabling self-optimizing, the query performance of all table formats remains stable as dynamic data is continuously written. ","description":"","title":"Benchmark Report","uri":"https://amoro.apache.org/benchmark-report/"},{"categories":null,"content":"Download Please choose an Amoro version to download from the following tables. It is recommended you use the latest release.\nThe latest release Coming soon!\nNon-Apache releases These releases were made before the Amoro project joined the ASF Incubator and have not followed the usual ASF release process.\nVersion Date Source AMS Flink Runtime Jars Spark Runtime Jars Trino Connector Release Notes 0.6.1 2024 Feb 21 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.15 Runtime Jar - Flink 1.16 Runtime Jar - Flink 1.17 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.6.0 2023 Nov 6 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.15 Runtime Jar - Flink 1.16 Runtime Jar - Flink 1.17 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.5.1 2023 Oct 10 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.5.0 2023 Aug 8 source - AMS(hadoop3) - AMS(hadoop2) - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 3.1 Runtime Jar\n- Spark 3.2 Runtime Jar\n- Spark 3.3 Runtime Jar Trino Connector release note 0.4.1 2023 Apr 3 source AMS - Flink 1.12 Runtime Jar - Flink 1.14 Runtime Jar - Flink 1.15 Runtime Jar - Spark 2.3 Runtime Jar\n- Spark 3.1 Runtime Jar Trino Connector release note 0.4.0 2022 Dec 6 source AMS - Flink 1.12 Runtime Jar\n- Flink 1.14 Runtime Jar\n- Flink 1.15 Runtime Jar - Spark 2.3 Runtime Jar\n- Spark 3.1 Runtime Jar Trino Connector release note ","description":"","title":"Download","uri":"https://amoro.apache.org/download/"},{"categories":null,"content":"Benchmark Guild This guilde introduces detailed steps for executing the benchmark to validate performance of various data lake formats.\nBy following the steps in the guild, you can learn about the analytical performance of different data lake table format. At the same time, you can flexibly adjust the test scenarios to obtain test results that better suit your actual scenario.\nDeploy testing environment Deploy by Docker With Docker-Compose, you can quickly set up an environment for performing the benchmark. The detailed steps reference: Lakehouse-benchmark.\nDeploy manually Alternatively, you can manually deploy the following components to set up the test environment：\nComponent Version Description Installation Guide MySQL 5.7+ MySQL is used to generate TPC-C data for synchronization to data lakes. MySQL Installation Guide Hadoop 2.3.7+ Hadoop is used to provide the storage for data lakes. Ambari Trino 380 Trino is used to execute TPC-H queries for Iceberg and Mixed-Iceberg format tables. Trino Installation Guide Amoro Trino Connector 0.4.0 To query Mixed-Iceberg Format tables in Trino, you need to install and configure the Amoro connector in Trino. Amoro Trino Connector Iceberg Trino Connector 0.13.0 To query Iceberg Format tables in Trino, you need to install and configure the Iceberg connector in Trino. Iceberg Trino Connector Presto 274 Presto is used to execute TPC-H queries for Hudi format tables. Presto Installation Guide Hudi Presto Connector 0.11.1 To query Iceberg Format tables in Trino, you need to install and configure the Iceberg connector in Presto. Hudi Presto Connector AMS 0.4.0 Amoro Management Service, support self-optimizing on tables during the test. AMS Installation Guide data-lake-benchmark 21 The core program of Benchmark which is responsible for generating test data, executing the testing process, and generating test results. Data Lake Benchmark lakehouse-benchmark-ingestion 1.0 Data synchronization tool based on Flink-CDC which can synchronize data from database to data lake in real-time. Lakehouse Benchmark Ingestion Benchmark steps Configure the configuration file config/mysql/sample_chbenchmark_config.xml file of program data-lake-benchmark. Fill in the information of MySQL and parameter scalefactor. scalefactor represents the number of warehouses, which controls the overall data volume. Generally, choose 10 or 100.\nGenerate static data into MySQL with command：\njava -jar lakehouse-benchmark.jar -b tpcc,chbenchmark -c config/mysql/sample_chbenchmark_config.xml --create=true --load=true Configure the configuration file config/ingestion-conf.yaml file of program lakehouse-benchmark-ingestion. Fill in the information of MySQL.\nStart the ingestion job to synchronize data form MySQL to data lake tables witch command:\njava -cp lakehouse-benchmark-ingestion-1.0-SNAPSHOT.jar com.netease.arctic.benchmark.ingestion.MainRunner -confDir [confDir] -sinkType [arctic/iceberg/hudi] -sinkDatabase [dbName] Execute TPC-H benchmark on static data with command: java -jar lakehouse-benchmark.jar -b chbenchmarkForTrino -c config/trino/trino_chbenchmark_config.xml --create=false --load=false --execute=true Execute TPC-C program to continuously write data into MYSQL witch command: java -jar lakehouse-benchmark.jar -b tpcc,chbenchmark -c config/mysql/sample_chbenchmark_config.xml --execute=true -s 5 Execute TPC-H benchmark on dynamic data with command: java -jar lakehouse-benchmark.jar -b chbenchmarkForTrino -c config/trino/trino_chbenchmark_config.xml --create=false --load=false --execute=true Obtain the benchmark results in the result directory of the data-lake-benchmark project.\nRepeat step 7 to obtain benchmark results for different points in time.\n","description":"","title":"How To Benchmark","uri":"https://amoro.apache.org/benchmark-guide/"},{"categories":null,"content":"Contributing to Amoro Thanks for your interest in the Amoro project. Contributions are welcome and are greatly appreciated! Every little effort helps, and credit will always be given.\nThis page provides some orientation and resources for getting involved with the project. It also offers recommendations for the best results when engaging with the community. We hope this will be a pleasant first experience for you and you’ll return to continue contributing.\nGet Involved If you have any questions, suggestions, or improvement ideas when using Amoro, you can participate in the Amoro community building through the following suggested channels.\nIssue Tracker - for tracking bugs, ideas, plans, etc. GitHub Discussions - second to the mailing list for anything else you want to share or ask. WeChat Group - add kllnn999 as a friend on WeChat and specify Amoro lover. You will be invited to join the Amoro WeChat group. Contributing Guide See Contributing for more details on contributing to Amoro.\n","description":"","title":"How To Contribute","uri":"https://amoro.apache.org/how-to-contribute/"},{"categories":null,"content":"Welcome to Apache Amoro (incubating) Amoro community is a free, open-source community project. Anyone interested in the Amoro project can join the community and contribute to its development by becoming a part of it.\nThis document describes some guidelines for joining the Amoro community.\nMailing Lists List Name Address Subscribe Unsubscribe Archive Developer List dev@amoro.apache.org subscribe unsubscribe archive Commits List commits@amoro.apache.org subscribe unsubscribe archive Roles and Responsibilities Amoro community is composed of and operated by the following roles:\nUser Contributor Committer PPMC User Community users, as defined by Amoro, are those members of the community who need the Amoro project, either individuals or businesses.\nContributor Everyone who contributes can become an Amoro contributor. The members will provide mentorship and guidance when new contributors need assistance.\nHow to become a Contributor? 1 merged PR in this project Responsibilities and privileges Actively participate in Amoro’s project development Participate in the project’s mailing lists and other communication channels, community events (meetups, hackathons, etc.) Learn and help others learn Amoro-related technologies Be listed as an Amoro contributor Committer Committers are promoted from Contributors. They have the authority to commit to the project’s repositories and are responsible for the planning and maintenance of Amoro. They are also active members who share their knowledge with the community.\nHow to become a Committer? Have a deep understanding of Amoro’s principles and future plans Have the ability to deal with various issues that arise in the project promptly Lead a major development, write and revise related documents Be voted in by the Amoro PPMC Responsibilities and privileges Mentor and guide other members in the community Ensure continued health of the project Looking after Amoro’s trademarks and branding Writing and submitting Incubator reports Be granted write access to Amoro repositories Be listed as an Amoro Committer and featured on the Amoro official website PPMC Members PPMC members are responsible for the planning and maintenance of Amoro. They are also active members who share their knowledge with the community.\nHow to become a PPMC member? Have the ability to deal with project issues Lead project development and iterations, and steer the overall direction of the project Be voted in by the Amoro PPMC Responsibilities and privileges Mentor and guide other members in the community Ensure continued health of the project, such as code quality and test coverage Make and approve technical design decisions Define milestones and releases Vote and promote new committers and PPMC members Be listed as an Amoro PPMC member and featured on the Amoro official website ","description":"","title":"Join Community","uri":"https://amoro.apache.org/join-community/"},{"categories":null,"content":"Quickstart This guide outlines the basic process of using Amoro, allowing you to quickly experience its core features. You can choose to use either the Iceberg Format or the Mixed-Iceberg Format to complete the entire process.\nIf you are more interested in the Mixed-Hive Format or the Paimon Format, you can refer to: Mixed-Hive Format and Paimon Format. For specific information on the different formats supported by Amoro, please refer to: Table Format.\nBefore starting the quick demo, some steps are required to prepare the environment. The fastest way to get started is to use a docker-compose file that uses the apache/amoro image. To use this, you’ll need to install the Docker CLI as well as the Docker Compose CLI.\nOnce you have those, save the yaml below into a file named docker-compose.yml:\nversion: \"3\" services: minio: image: minio/minio container_name: minio environment: - MINIO_ROOT_USER=admin - MINIO_ROOT_PASSWORD=password - MINIO_DOMAIN=minio networks: amoro_network: aliases: - warehouse.minio ports: - 9001:9001 - 9000:9000 command: [ \"server\", \"/data\", \"--console-address\", \":9001\" ] mc: depends_on: - minio image: minio/mc container_name: mc networks: amoro_network: environment: - AWS_ACCESS_KEY_ID=admin - AWS_SECRET_ACCESS_KEY=password - AWS_REGION=us-east-1 entrypoint: \u003e /bin/sh -c \" until (/usr/bin/mc config host add minio http://minio:9000 admin password) do echo '...waiting...' \u0026\u0026 sleep 1; done; /usr/bin/mc rm -r --force minio/warehouse; /usr/bin/mc mb minio/warehouse; /usr/bin/mc policy set public minio/warehouse; tail -f /dev/null \" amoro: image: apache/amoro container_name: amoro ports: - 8081:8081 - 1630:1630 - 1260:1260 environment: - JVM_XMS=1024 networks: amoro_network: aliases: - warehouse.minio volumes: - ./amoro:/tmp/warehouse command: [\"/entrypoint.sh\", \"ams\"] tty: true stdin_open: true networks: amoro_network: driver: bridge Next, start up the docker containers with this command:\ndocker-compose up Prepare steps Create optimizer group Open http://localhost:1630 in a browser, enter admin/admin to log in to the dashboard.\nClick on Optimizing in the sidebar, choose Optimizer Groups and click Add Group button to create a new group befre creating catalog:\nCreate catalog Click on Catalogs in the sidebar, click on the + button under Catalog List to create a test catalog, and name it to demo_catalog:\nIceberg Format Mixed-Iceberg Format To use the Iceberg Format, select Type as Internal Catalog, and choose Iceberg as Table Format. To use the Mixed-Iceberg Format, select Type as Internal Catalog, and choose Mixed-Iceberg as Table Format. Start optimizers Click on Optimizing in the sidebar, select the Optimizer Group tab, and click the scale-out operation for group local.\nSet the concurrency of the optimizer to 1 and click OK.\nThen you can switch the tab to Optimizers, you can find the newly launched optimizer here.\nYou may need to wait for up to 30 seconds for the optimizer to register with AMS. Demo setps Initialize tables Click on Terminal in the sidebar, you can create the test tables here using SQL. Terminal supports executing Spark SQL statements for now.\nIceberg Format Mixed-Iceberg Format CREATE DATABASE IF NOT EXISTS db; CREATE TABLE IF NOT EXISTS db.user ( id INT, name string, ts TIMESTAMP ) USING iceberg PARTITIONED BY (days(ts)); INSERT OVERWRITE db.user VALUES (1, \"eric\", timestamp(\"2022-07-01 12:32:00\")), (2, \"frank\", timestamp(\"2022-07-02 09:11:00\")), (3, \"lee\", timestamp(\"2022-07-02 10:11:00\")); SELECT * FROM db.user; CREATE DATABASE IF NOT EXISTS db; CREATE TABLE IF NOT EXISTS db.user ( id INT, name string, ts TIMESTAMP, PRIMARY KEY(id) ) USING mixed_iceberg PARTITIONED BY (days(ts)); INSERT OVERWRITE db.user VALUES (1, \"eric\", timestamp(\"2022-07-01 12:32:00\")), (2, \"frank\", timestamp(\"2022-07-02 09:11:00\")), (3, \"lee\", timestamp(\"2022-07-02 10:11:00\")); SELECT * FROM db.user; Click on the RUN button uppon the SQL editor, and wait for the SQL query to finish executing. You can then see the query results under the SQL editor.\nMake some changes Execute the following SQL statements one by one in the Terminal:\n-- insert a few rows first INSERT INTO db.user (id, name, ts) VALUES (4, 'rock', CAST('2022-07-02 01:11:20' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (5, 'jack', CAST('2022-07-02 05:22:10' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (6, 'mars', CAST('2022-07-02 08:23:20' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (7, 'cloe', CAST('2022-07-02 08:44:50' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (8, 'smith', CAST('2022-07-02 10:52:20' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (9, 'piec', CAST('2022-07-02 11:24:30' AS TIMESTAMP)); INSERT INTO db.user (id, name, ts) VALUES (10, 'vovo', CAST('2022-07-02 12:00:20' AS TIMESTAMP)); -- delete some rows then DELETE FROM db.user where id = 1; DELETE FROM db.user where id = 4; DELETE FROM db.user where id = 7; -- query the table SELECT * from db.user; Check self-optimizing As new data is written to the table, Amoro will automatically trigger self-optimizing on the table.\nClick on Tables in the sidebar, select the test table to enter the table details page, switch to the Optimizing tab, where you can see all the self-optimizing tasks on the table.\nYou can also enter the Optimizing page through the the sidebar to view the current optimizing status of all tables.\nFor more information on Self-Optimizing, please refer to: Self-optimizing\nAfter finishing the demo, you can run the following command in the directory of docker-compose.yml to stop all containers: docker-compose down ","description":"","title":"Quickstart","uri":"https://amoro.apache.org/quick-start/"},{"categories":null,"content":"Roadmap This roadmap displays the workings that the Amoro community plans to complete in 2024 and their priorities. Each item is associated with a Github issue, where you can learn about its specific design and latest developments.\nPriority 1 Refactor AMS using function and process api Add metric collection for ams Support table sorting process Serverless process: automatic scaling for process resources Monitor the health status of tables Improve Mixed format Priority 2 Support Apache Hudi Support ingestion process Maintain secondary indexes on tables Collect scan metrics and improve optimizing rules based on them Enhance the management capabilities of the Paimon format ","description":"","title":"Roadmap","uri":"https://amoro.apache.org/roadmap/"}]