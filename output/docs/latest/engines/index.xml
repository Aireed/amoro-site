<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Engines on Amoro</title>
    <link>https://amoro.apache.org/docs/latest/engines/</link>
    <description>Recent content in Engines on Amoro</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://amoro.apache.org/docs/latest/engines/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink DataStream</title>
      <link>https://amoro.apache.org/docs/latest/flink-datastream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-datastream/</guid>
      <description>Flink DataStream Add maven dependency To add a dependency on Mixed-format flink connector in Maven, add the following to your pom.xml:&#xA;&amp;lt;dependencies&amp;gt; ... &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;org.apache.amoro&amp;lt;/groupId&amp;gt; &amp;lt;!-- For example: amoro-mixed-format-flink-runtime-1.15 --&amp;gt; &amp;lt;artifactId&amp;gt;amoro-mixed-format-flink-runtime-${flink.minor-version}&amp;lt;/artifactId&amp;gt; &amp;lt;!-- For example: 0.7.0-incubating --&amp;gt; &amp;lt;version&amp;gt;${amoro-mixed-format-flink.version}&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; ... &amp;lt;/dependencies&amp;gt; Reading with DataStream Amoro supports reading data in Batch or Streaming mode through Java API.&#xA;Batch mode Using Batch mode to read the full and incremental data in the FileStore.</description>
    </item>
    <item>
      <title>Flink DDL</title>
      <link>https://amoro.apache.org/docs/latest/flink-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-ddl/</guid>
      <description>Flink DDL Create catalogs Flink SQL The following statement can be executed to create a Flink catalog:&#xA;CREATE CATALOG &amp;lt;catalog_name&amp;gt; WITH ( &amp;#39;type&amp;#39;=&amp;#39;mixed_iceberg&amp;#39;, `&amp;lt;config_key&amp;gt;`=`&amp;lt;config_value&amp;gt;` ); Where &amp;lt;catalog_name&amp;gt; is the user-defined name of the Flink catalog, and &amp;lt;config_key&amp;gt;=&amp;lt;config_value&amp;gt; has the following configurations:&#xA;Key Default Value Type Required Description type N/A String Yes Catalog type, validate values are mixed_iceberg and mixed_hive metastore.url (none) String Yes The URL for Amoro Metastore is thrift://&amp;lt;ip&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;catalog_name_in_metastore&amp;gt;.</description>
    </item>
    <item>
      <title>Flink DML</title>
      <link>https://amoro.apache.org/docs/latest/flink-dml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-dml/</guid>
      <description>Flink DML Querying with SQL Amoro tables support reading data in stream or batch mode through Flink SQL. You can switch modes using the following methods:&#xA;-- Run Flink tasks in streaming mode in the current session SET execution.runtime-mode = streaming; -- Run Flink tasks in batch mode in the current session SET execution.runtime-mode = batch; The following Hint Options are supported:&#xA;Key Default Value Type Required Description source.parallelism (none) Integer No Defines a custom parallelism for the source.</description>
    </item>
    <item>
      <title>Flink Getting Started</title>
      <link>https://amoro.apache.org/docs/latest/flink-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-getting-started/</guid>
      <description>Flink Getting Started Iceberg format The Iceberg Format can be accessed using the Connector provided by Iceberg. Refer to the documentation at Iceberg Flink user manual for more information.&#xA;Paimon format The Paimon Format can be accessed using the Connector provided by Paimon. Refer to the documentation at Paimon Flink user manual for more information.&#xA;Mixed format The Apache Flink engine can process Amoro table data in batch and streaming mode.</description>
    </item>
    <item>
      <title>Spark Configuration</title>
      <link>https://amoro.apache.org/docs/latest/spark-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-configuration/</guid>
      <description>Spark Configuration Catalogs configuration Using Mixed-Format in a standalone catalog Starting from version 3.x, Spark supports configuring an independent Catalog. If you want to use a Mixed-Format table in a standalone Catalog, you can configure it as follows:&#xA;spark.sql.catalog.mixed_catalog=org.apache.amoro.spark.MixedFormatSparkCatalog spark.sql.catalog.mixed_catalog.url=thrift://${AMS_HOST}:${AMS_PORT}/${AMS_CATALOG_NAME_HIVE} Then, execute the following SQL in the Spark SQL Client to switch to the corresponding catalog.&#xA;use mixed_catalog; Of course, you can also access Mixed-Format tables by directly using the triplet mixed_catalog.</description>
    </item>
    <item>
      <title>Spark DDL</title>
      <link>https://amoro.apache.org/docs/latest/spark-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-ddl/</guid>
      <description>Spark DDL CREATE TABLE To create an MixedFormat table under an Amoro Catalog, you can use using mixed_iceberg or using mixed_hive to specify the provider in the CREATE TABLE statement. If the Catalog type is Hive, the created table will be a Hive-compatible table.&#xA;CREATE TABLE mixed_catalog.db.sample ( id bigint COMMENT &amp;#34;unique id&amp;#34;, data string ) USING mixed_iceberg PRIMARY KEY You can use PRIMARY KEY in the CREATE TABLE statement to specify the primary key column.</description>
    </item>
    <item>
      <title>Spark Getting Started</title>
      <link>https://amoro.apache.org/docs/latest/spark-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-getting-started/</guid>
      <description>Spark Getting Started Iceberg Format The Iceberg Format can be accessed using the Connector provided by Iceberg. Refer to the documentation at Iceberg Spark Connector for more information.&#xA;Paimon Format The Paimon Format can be accessed using the Connector provided by Paimon. Refer to the documentation at Paimon Spark Connector for more information.&#xA;Mixed Format To use Amoro in a Spark shell, use the &amp;ndash;packages option:&#xA;spark-shell --packages org.apache.amoro:amoro-mixed-spark-3.3-runtime:0.7.0 If you want to include the connector in your Spark installation, add the amoro-mixed-spark-3.</description>
    </item>
    <item>
      <title>Spark Queries</title>
      <link>https://amoro.apache.org/docs/latest/spark-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-queries/</guid>
      <description>Spark Queries Querying with SQL Querying Mixed-Format table by merge on read Using Select statement to query on Mixed-Format tables.&#xA;SELECT * FROM mixed_catalog.db.sample The Mixed-Format connector will merge the data from BaseStore and ChangeStore.&#xA;Query on change store For a Mixed-Format table with primary keys. you can query on ChangeStore by .change.&#xA;SELECT * FROM mixed_catalog.db.sample.change +---+----+----+---------------+------------+--------------+ | id|name|data|_transaction_id|_file_offset|_change_action| +---+----+----+---------------+------------+--------------+ | 1|dddd|abcd| 3| 1| INSERT| | 1|dddd|abcd| 3| 2| DELETE| +---+----+----+---------------+------------+--------------+ The addition columns are:</description>
    </item>
    <item>
      <title>Spark Writes</title>
      <link>https://amoro.apache.org/docs/latest/spark-writes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/spark-writes/</guid>
      <description>Spark Writes Writing with SQL INSERT OVERWRITE INSERT OVERWRITE can replace the partition in a table with the results of a query.&#xA;The default overwrite mode of Spark is Static, you can change the overwrite mode by&#xA;SET spark.sql.sources.partitionOverwriteMode=dynamic To demonstrate the behavior of dynamic and static overwrites, a test table is defined using the following DDL:&#xA;CREATE TABLE mixed_catalog.db.sample ( id int, data string, ts timestamp, primary key (id)) USING mixed_iceberg PARTITIONED BY (days(ts)) When Spark&amp;rsquo;s overwrite mode is dynamic, the partitions of the rows generated by the SELECT query will be replaced.</description>
    </item>
    <item>
      <title>Trino</title>
      <link>https://amoro.apache.org/docs/latest/trino/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/trino/</guid>
      <description>Trino Iceberg format Iceberg format can be accessed using the Iceberg Connector provided by Trino. please refer to the documentation at Iceberg Trino user manual for more information.&#xA;Paimon format Paimon format can be accessed using the Paimon Connector provided by Trino. please refer to the documentation at Paimon Trino user manual for more information.&#xA;Mixed format Install Create the {trino_home}/plugin/amoro directory in the Trino installation package, and extract the contents of the amoro-trino package amoro-mixed-format-trino-xx.</description>
    </item>
    <item>
      <title>Using Logstore</title>
      <link>https://amoro.apache.org/docs/latest/flink-using-logstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/latest/flink-using-logstore/</guid>
      <description>Using Logstore Due to the limitations of traditional offline data warehouse architectures in supporting real-time business needs, real-time data warehousing has experienced rapid evolution in recent years. In the architecture of real-time data warehousing, Apache Kafka is often used as the storage system for real-time data. However, this also brings about the issue of data disconnection between offline data warehouses.&#xA;Developers often need to pay attention to data stored in HDFS as well as data in Kafka, which increases the complexity of business development.</description>
    </item>
  </channel>
</rss>
