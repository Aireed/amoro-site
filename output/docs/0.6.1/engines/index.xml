<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Engines on Amoro</title>
    <link>https://amoro.apache.org/docs/0.6.1/engines/</link>
    <description>Recent content in Engines on Amoro</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://amoro.apache.org/docs/0.6.1/engines/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Flink DataStream</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-datastream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-datastream/</guid>
      <description>&lt;h1 id=&#34;flink-datastream&#34;&gt;Flink DataStream&lt;/h1&gt;&#xA;&lt;h2 id=&#34;reading-with-datastream&#34;&gt;Reading with DataStream&lt;/h2&gt;&#xA;&lt;p&gt;Amoro supports reading data in Batch or Streaming mode through Java API.&lt;/p&gt;&#xA;&lt;h3 id=&#34;batch-mode&#34;&gt;Batch mode&lt;/h3&gt;&#xA;&lt;p&gt;Using Batch mode to read the full and incremental data in the FileStore.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Non-primary key tables support reading full data in batch mode, snapshot data with a specified snapshot-id or timestamp, and incremental data with a specified snapshot interval.&lt;/li&gt;&#xA;&lt;li&gt;The primary key table temporarily only supports reading the current full amount and later CDC data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;StreamExecutionEnvironment env &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; StreamExecutionEnvironment.&lt;span style=&#34;color:#a6e22e&#34;&gt;createLocalEnvironment&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;InternalCatalogBuilder catalogBuilder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    InternalCatalogBuilder&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;builder&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;metastoreUrl&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;thrift://&amp;lt;url&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;catalog_name&amp;gt;&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;TableIdentifier tableId &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TableIdentifier.&lt;span style=&#34;color:#a6e22e&#34;&gt;of&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;database_name&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test_table&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;AmoroTableLoader tableLoader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AmoroTableLoader.&lt;span style=&#34;color:#a6e22e&#34;&gt;of&lt;/span&gt;(tableId, catalogBuilder);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Map&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;String, String&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; properties &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; HashMap&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//  Default is true.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;properties.&lt;span style=&#34;color:#a6e22e&#34;&gt;put&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;streaming&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;false&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DataStream&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;RowData&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    FlinkSource.&lt;span style=&#34;color:#a6e22e&#34;&gt;forRowData&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;env&lt;/span&gt;(env)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;tableLoader&lt;/span&gt;(tableLoader)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// The primary key table only supports reading the current full amount and later CDC data temporarily, without the properties parameter .&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;properties&lt;/span&gt;(properties)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;build&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// print All data read&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;batch.&lt;span style=&#34;color:#a6e22e&#34;&gt;print&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Submit and execute the task&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;env.&lt;span style=&#34;color:#a6e22e&#34;&gt;execute&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test Amoro Batch Read&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The map properties contain below keys, &lt;strong&gt;currently only valid for non-primary key tables&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink DDL</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-ddl/</guid>
      <description>&lt;h1 id=&#34;flink-ddl&#34;&gt;Flink DDL&lt;/h1&gt;&#xA;&lt;h2 id=&#34;create-catalogs&#34;&gt;Create catalogs&lt;/h2&gt;&#xA;&lt;h3 id=&#34;flink-sql&#34;&gt;Flink SQL&lt;/h3&gt;&#xA;&lt;p&gt;The following statement can be executed to create a Flink catalog:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;CATALOG&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;catalog_name&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WITH&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;arctic&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;`&amp;lt;&lt;/span&gt;config_key&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;`=`&amp;lt;&lt;/span&gt;config_value&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;); &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Where &lt;code&gt;&amp;lt;catalog_name&amp;gt;&lt;/code&gt; is the user-defined name of the Flink catalog, and &lt;code&gt;&amp;lt;config_key&amp;gt;&lt;/code&gt;=&lt;code&gt;&amp;lt;config_value&amp;gt;&lt;/code&gt; has the following configurations:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Key&lt;/th&gt;&#xA;          &lt;th&gt;Default Value&lt;/th&gt;&#xA;          &lt;th&gt;Type&lt;/th&gt;&#xA;          &lt;th&gt;Required&lt;/th&gt;&#xA;          &lt;th&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;metastore.url&lt;/td&gt;&#xA;          &lt;td&gt;(none)&lt;/td&gt;&#xA;          &lt;td&gt;String&lt;/td&gt;&#xA;          &lt;td&gt;Yes&lt;/td&gt;&#xA;          &lt;td&gt;The URL for Amoro Metastore is thrift://&lt;code&gt;&amp;lt;ip&amp;gt;&lt;/code&gt;:&lt;code&gt;&amp;lt;port&amp;gt;&lt;/code&gt;/&lt;code&gt;&amp;lt;catalog_name_in_metastore&amp;gt;&lt;/code&gt;.&lt;br&gt;If high availability is enabled for AMS, it can also be specified in the form of zookeeper://{zookeeper-server}/{cluster-name}/{catalog-name}.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;default-database&lt;img width=100/&gt;&lt;/td&gt;&#xA;          &lt;td&gt;default&lt;/td&gt;&#xA;          &lt;td&gt;String&lt;/td&gt;&#xA;          &lt;td&gt;No&lt;/td&gt;&#xA;          &lt;td&gt;The default database to use&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;property-version&lt;/td&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Integer&lt;/td&gt;&#xA;          &lt;td&gt;No&lt;/td&gt;&#xA;          &lt;td&gt;Catalog properties version, this option is for future backward compatibility&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;The authentication information of AMS catalog can upload configuration files on AMS website,&#xA;or specify the authentication information and configuration file paths when creating catalogs with Flink DDL&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink DML</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-dml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-dml/</guid>
      <description>&lt;h1 id=&#34;flink-dml&#34;&gt;Flink DML&lt;/h1&gt;&#xA;&lt;h2 id=&#34;querying-with-sql&#34;&gt;Querying with SQL&lt;/h2&gt;&#xA;&lt;p&gt;Amoro tables support reading data in stream or batch mode through Flink SQL. You can switch modes using the following methods:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;-- Run Flink tasks in streaming mode in the current session&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; execution.runtime&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; streaming;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;-- Run Flink tasks in batch mode in the current session&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; execution.runtime&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;batch-mode&#34;&gt;Batch mode&lt;/h3&gt;&#xA;&lt;p&gt;Use batch mode to read full and incremental data from FileStore.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TIPS&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;LogStore does not support bounded reading.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink Getting Started</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-getting-started/</guid>
      <description>&lt;h1 id=&#34;flink-getting-started&#34;&gt;Flink Getting Started&lt;/h1&gt;&#xA;&lt;h2 id=&#34;iceberg-format&#34;&gt;Iceberg format&lt;/h2&gt;&#xA;&lt;p&gt;The Iceberg Format can be accessed using the Connector provided by Iceberg.&#xA;Refer to the documentation at &lt;a href=&#34;https://iceberg.apache.org/docs/latest/flink-connector/&#34;&gt;Iceberg Flink user manual&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;paimon-format&#34;&gt;Paimon format&lt;/h2&gt;&#xA;&lt;p&gt;The Paimon Format can be accessed using the Connector provided by Paimon.&#xA;Refer to the documentation at &lt;a href=&#34;https://paimon.apache.org/docs/master/engines/flink/&#34;&gt;Paimon Flink user manual&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;mixed-format&#34;&gt;Mixed format&lt;/h2&gt;&#xA;&lt;p&gt;The Apache Flink engine can process Amoro table data in batch and streaming mode. The Flink on Amoro connector provides the ability to read and write to the Amoro data lake while ensuring data consistency. To meet the high real-time data requirements of businesses, the Amoro data lake&amp;rsquo;s underlying storage structure is designed with LogStore, which stores the latest changelog or append-only real-time data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Configuration</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-configuration/</guid>
      <description>&lt;h1 id=&#34;spark-configuration&#34;&gt;Spark Configuration&lt;/h1&gt;&#xA;&lt;h2 id=&#34;catalogs-configuration&#34;&gt;Catalogs configuration&lt;/h2&gt;&#xA;&lt;h3 id=&#34;using-mixed-format-in-a-standalone-catalog&#34;&gt;Using Mixed-Format in a standalone catalog&lt;/h3&gt;&#xA;&lt;p&gt;Starting from version 3.x, Spark supports configuring an independent Catalog.&#xA;If you want to use a Mixed-Format table in a standalone Catalog, you can configure it as follows:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;spark.sql.catalog.arctic_catalog&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;com.netease.arctic.spark.ArcticSparkCatalog&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;spark.sql.catalog.arctic_catalog.url&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;thrift://${AMS_HOST}:${AMS_PORT}/${AMS_CATALOG_NAME_HIVE}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, execute the following SQL in the Spark SQL Client to switch to the corresponding catalog.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;use arctic_catalog;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Of course, you can also access Mixed-Format tables by directly using the triplet&#xA;&lt;code&gt;arctic_catalog.{db_name}.{table_name}&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark DDL</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-ddl/</guid>
      <description>&lt;h1 id=&#34;spark-ddl&#34;&gt;Spark DDL&lt;/h1&gt;&#xA;&lt;h2 id=&#34;create-table&#34;&gt;CREATE TABLE&lt;/h2&gt;&#xA;&lt;p&gt;To create an MixedFormat table under an Amoro Catalog, you can use &lt;code&gt;USING ARCTIC&lt;/code&gt; to specify the provider in the&#xA;&lt;code&gt;CREATE TABLE&lt;/code&gt; statement. If the Catalog type is Hive, the created table will be a Hive-compatible table.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; arctic_catalog.db.sample (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    id bigint  &lt;span style=&#34;color:#66d9ef&#34;&gt;COMMENT&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;unique id&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;data&lt;/span&gt; string&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;USING&lt;/span&gt; arctic &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;primary-key&#34;&gt;PRIMARY KEY&lt;/h3&gt;&#xA;&lt;p&gt;You can use &lt;code&gt;PRIMARY KEY&lt;/code&gt; in the &lt;code&gt;CREATE TABLE&lt;/code&gt; statement to specify the primary key column.&#xA;MixedFormat ensures the uniqueness of the primary key column through MOR (Merge on Read) and Self-Optimizing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Getting Started</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-getting-started/</guid>
      <description>&lt;h1 id=&#34;spark-getting-started&#34;&gt;Spark Getting Started&lt;/h1&gt;&#xA;&lt;h1 id=&#34;iceberg-format&#34;&gt;Iceberg Format&lt;/h1&gt;&#xA;&lt;p&gt;The Iceberg Format can be accessed using the Connector provided by Iceberg.&#xA;Refer to the documentation at &lt;a href=&#34;https://iceberg.apache.org/docs/latest/getting-started/&#34;&gt;Iceberg Spark Connector&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h1 id=&#34;paimon-format&#34;&gt;Paimon Format&lt;/h1&gt;&#xA;&lt;p&gt;The Paimon Format can be accessed using the Connector provided by Paimon.&#xA;Refer to the documentation at &lt;a href=&#34;https://paimon.apache.org/docs/master/engines/spark3/&#34;&gt;Paimon Spark Connector&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h1 id=&#34;mixed-format&#34;&gt;Mixed Format&lt;/h1&gt;&#xA;&lt;p&gt;To use Amoro in a Spark shell, use the &amp;ndash;packages option:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark-shell --packages com.netease.amoro:amoro-spark-3.3-runtime:0.5.0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;If you want to include the connector in your Spark installation, add the &lt;code&gt;amoro-spark-3.3-runtime&lt;/code&gt; Jar to&#xA;Spark&amp;rsquo;s &lt;code&gt;jars&lt;/code&gt; folder.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Queries</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-queries/</guid>
      <description>&lt;h1 id=&#34;spark-queries&#34;&gt;Spark Queries&lt;/h1&gt;&#xA;&lt;h2 id=&#34;querying-with-sql&#34;&gt;Querying with SQL&lt;/h2&gt;&#xA;&lt;h3 id=&#34;querying-mixed-format-table-by-merge-on-read&#34;&gt;Querying Mixed-Format table by merge on read&lt;/h3&gt;&#xA;&lt;p&gt;Using &lt;code&gt;Select&lt;/code&gt; statement to query on Mixed-Format tables.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; arctic_catalog.db.sample&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The Mixed-Format connector will merge the data from &lt;code&gt;BaseStore&lt;/code&gt; and &lt;code&gt;ChangeStore&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;query-on-change-store&#34;&gt;Query on change store&lt;/h3&gt;&#xA;&lt;p&gt;For a Mixed-Format table with primary keys. you can query on &lt;code&gt;ChangeStore&lt;/code&gt; by &lt;code&gt;.change&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; arctic_catalog.db.sample.change&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;---+----+----+---------------+------------+--------------+&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; id&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;name&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;data&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_transaction_id&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_file_offset&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_change_action&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;---+----+----+---------------+------------+--------------+&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;dddd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;abcd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;              &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;INSERT&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;dddd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;abcd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;              &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;DELETE&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;---+----+----+---------------+------------+--------------+&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The addition columns are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;_transaction_id: The transaction ID allocated by AMS during data write is assigned per SQL execution in batch mode and&#xA;per checkpoint in streaming mode.&lt;/li&gt;&#xA;&lt;li&gt;_file_offset：The order of data written with the same &lt;code&gt;_transaction_id&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;_change_action：The type of change record, &lt;code&gt;INSERT&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;querying-with-dataframes&#34;&gt;Querying with DataFrames&lt;/h2&gt;&#xA;&lt;p&gt;You can read the Mixed-Format table by Spark DataFrames:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Writes</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-writes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-writes/</guid>
      <description>&lt;h1 id=&#34;spark-writes&#34;&gt;Spark Writes&lt;/h1&gt;&#xA;&lt;h2 id=&#34;writing-with-sql&#34;&gt;Writing with SQL&lt;/h2&gt;&#xA;&lt;h3 id=&#34;insert-overwrite&#34;&gt;INSERT OVERWRITE&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;INSERT OVERWRITE&lt;/code&gt; can replace the partition in a table with the results of a query.&lt;/p&gt;&#xA;&lt;p&gt;The default overwrite mode of Spark is &lt;code&gt;Static&lt;/code&gt;, you can change the overwrite mode by&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SET spark.sql.sources.partitionOverwriteMode=dynamic&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To demonstrate the behavior of dynamic and static overwrites, a test table is defined using the following DDL:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; arctic_catalog.db.sample (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    id int,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;data&lt;/span&gt; string,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ts &lt;span style=&#34;color:#66d9ef&#34;&gt;timestamp&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;primary&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;key&lt;/span&gt; (id))&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;USING&lt;/span&gt; arctic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;PARTITIONED &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; (days(ts))&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When Spark&amp;rsquo;s overwrite mode is dynamic, the partitions of the rows generated by the SELECT query will be replaced.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trino</title>
      <link>https://amoro.apache.org/docs/0.6.1/trino/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/trino/</guid>
      <description>&lt;h1 id=&#34;trino&#34;&gt;Trino&lt;/h1&gt;&#xA;&lt;h2 id=&#34;iceberg-format&#34;&gt;Iceberg format&lt;/h2&gt;&#xA;&lt;p&gt;Iceberg format can be accessed using the Iceberg Connector provided by Trino.&#xA;please refer to the documentation at &lt;a href=&#34;https://trino.io/docs/current/connector/iceberg.html#&#34;&gt;Iceberg Trino user manual&lt;/a&gt; for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;paimon-format&#34;&gt;Paimon format&lt;/h2&gt;&#xA;&lt;p&gt;Paimon format can be accessed using the Paimon Connector provided by Trino.&#xA;please refer to the documentation at &lt;a href=&#34;https://paimon.apache.org/docs/master/engines/trino/&#34;&gt;Paimon Trino user manual&lt;/a&gt; for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;mixed-format&#34;&gt;Mixed format&lt;/h2&gt;&#xA;&lt;h3 id=&#34;install&#34;&gt;Install&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create the {trino_home}/plugin/amoro directory in the Trino installation package,&#xA;and extract the contents of the amoro-trino package trino-amoro-xx-SNAPSHOT.tar.gz to the {trino_home}/plugin/amoro directory.&lt;/li&gt;&#xA;&lt;li&gt;Configure the Catalog configuration file for Amoro in the {trino_home}/etc/catalog directory, for example:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-tex&#34; data-lang=&#34;tex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;connector.name=arctic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;arctic.url=thrift://{ip}:{port}/{catalogName}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;Configure the JVM configuration file for Trino in the {trino_home}/etc directory named &lt;code&gt;jvm.config&lt;/code&gt; :&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-tex&#34; data-lang=&#34;tex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;--add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;support-sql-statement&#34;&gt;Support SQL statement&lt;/h3&gt;&#xA;&lt;h4 id=&#34;query-table&#34;&gt;Query Table&lt;/h4&gt;&#xA;&lt;p&gt;By adopting the Merge-On-Read approach to read Mixed Format, the latest data of the table can be read, for example:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstore</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-using-logstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-using-logstore/</guid>
      <description>&lt;h1 id=&#34;using-logstore&#34;&gt;Using Logstore&lt;/h1&gt;&#xA;&lt;p&gt;Due to the limitations of traditional offline data warehouse architectures in supporting real-time business needs, real-time data warehousing has experienced rapid evolution in recent years. In the architecture of real-time data warehousing, Apache Kafka is often used as the storage system for real-time data. However, this also brings about the issue of data disconnection between offline data warehouses.&lt;/p&gt;&#xA;&lt;p&gt;Developers often need to pay attention to data stored in HDFS as well as data in Kafka, which increases the complexity of business development. Therefore, Amoro proposes the addition of an optional parameter, &amp;ldquo;LogStore enabled&amp;rdquo; (&lt;code&gt;log-store.enabled&lt;/code&gt;), to the table configuration. This allows for retrieving data with sub-second and minute-level latency by operating on a single table while ensuring the eventual consistency of data from both sources.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
