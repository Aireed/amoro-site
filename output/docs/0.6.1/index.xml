<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Introduction on Amoro</title>
    <link>https://amoro.apache.org/docs/0.6.1/</link>
    <description>Recent content in Introduction on Amoro</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <atom:link href="https://amoro.apache.org/docs/0.6.1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Paimon</title>
      <link>https://amoro.apache.org/docs/0.6.1/paimon-format/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/paimon-format/</guid>
      <description>&lt;h1 id=&#34;paimon-format&#34;&gt;Paimon Format&lt;/h1&gt;&#xA;&lt;p&gt;Paimon format refers to &lt;a href=&#34;https://paimon.apache.org/&#34;&gt;Apache Paimon&lt;/a&gt; table.&#xA;Paimon is a streaming data lake platform with high-speed data ingestion, changelog tracking and efficient real-time analytics.&lt;/p&gt;&#xA;&lt;p&gt;By registering Paimon&amp;rsquo;s catalog with Amoro, users can view information such as Schema, Options, Files, Snapshots, DDLs, Compaction information, and more for Paimon tables.&#xA;Furthermore, they can operate on Paimon tables using Spark SQL in the Terminal. The current supported catalog types and file system types for Paimon are all supported.&lt;/p&gt;</description>
    </item>
    <item>
      <title></title>
      <link>https://amoro.apache.org/docs/0.6.1/readme/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/readme/</guid>
      <description>&lt;h2 id=&#34;amoro-docs&#34;&gt;Amoro Docs&lt;/h2&gt;&#xA;&lt;p&gt;This directory contains the documentation content of Amoro.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;The documentation is written in Markdown format.&lt;/li&gt;&#xA;&lt;li&gt;The images referenced in the documentation are saved in the &lt;code&gt;images&lt;/code&gt; folder.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;style&#34;&gt;Style&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Proper nouns should start with a capital letter, like Hadoop、Hive、Iceberg、Amoro&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Catalogs</title>
      <link>https://amoro.apache.org/docs/0.6.1/catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/catalogs/</guid>
      <description>&lt;h1 id=&#34;catalogs&#34;&gt;Catalogs&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduce-multi-catalog&#34;&gt;Introduce multi-catalog&lt;/h2&gt;&#xA;&lt;p&gt;A catalog is a metadata namespace that stores information about databases, tables, views, indexes, users, and UDFs. It provides a higher-level&#xA;namespace for &lt;code&gt;table&lt;/code&gt; and &lt;code&gt;database&lt;/code&gt;. Typically, a catalog is associated with a specific type of data source or cluster. In Flink, Spark and Trino,&#xA;the multi-catalog feature can be used to support SQL across data sources, such as:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-SQL&#34; data-lang=&#34;SQL&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;c&lt;/span&gt;.ID, &lt;span style=&#34;color:#66d9ef&#34;&gt;c&lt;/span&gt;.NAME, &lt;span style=&#34;color:#66d9ef&#34;&gt;c&lt;/span&gt;.AGE, o.AMOUNT&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; MYSQL.ONLINE.CUSTOMERS &lt;span style=&#34;color:#66d9ef&#34;&gt;c&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;JOIN&lt;/span&gt; HIVE.OFFLINE.ORDERS o&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ON&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;c&lt;/span&gt;.ID &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; o.CUSTOMER_ID)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In the past, data lakes were managed using the Hive Metastore (HMS) to handle metadata. Unfortunately, HMS does not support multi-catalog, which&#xA;limits the capabilities of engines on the data lake. For example, some users may want to use Spark to perform federated computation across different&#xA;Hive clusters by specifying the catalog name, requiring them to develop a Hive catalog plugin in the upper layer. Additionally, data lake formats are&#xA;moving from a single Hive-centric approach to a landscape of competing formats such as Iceberg, Delta, and Hudi. These new data lake formats are more&#xA;cloud-friendly and will facilitate the migration of data lakes to the cloud. In this context, a management system that supports multi-catalog is&#xA;needed to help users govern data lakes with different environments and formats.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CDC Ingestion</title>
      <link>https://amoro.apache.org/docs/0.6.1/cdc-ingestion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/cdc-ingestion/</guid>
      <description>&lt;h1 id=&#34;cdc-ingestion&#34;&gt;CDC Ingestion&lt;/h1&gt;&#xA;&lt;p&gt;CDC stands for Change Data Capture, which is a broad concept, as long as it can capture the change data, it can be called CDC. &lt;a href=&#34;https://github.com/ververica/flink-cdc-connectors&#34;&gt;Flink CDC&lt;/a&gt; is a Log message-based data capture tool, all the inventory and incremental data can be captured. Taking MySQL as an example, it can easily capture Binlog data through Debezium and process the calculations in real time to send them to the data lake. The data lake can then be queried by other engines.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Configurations</title>
      <link>https://amoro.apache.org/docs/0.6.1/configurations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/configurations/</guid>
      <description>&lt;h1 id=&#34;table-configurations&#34;&gt;Table Configurations&lt;/h1&gt;&#xA;&lt;h2 id=&#34;multi-level-configuration-management&#34;&gt;Multi-level configuration management&lt;/h2&gt;&#xA;&lt;p&gt;Amoro provides configurations that can be configured at the &lt;code&gt;Catalog&lt;/code&gt;, &lt;code&gt;Table&lt;/code&gt;, and &lt;code&gt;Engine&lt;/code&gt; levels. The configuration&#xA;priority is given first to the &lt;code&gt;Engine&lt;/code&gt;, followed by the &lt;code&gt;Table&lt;/code&gt;, and finally by the &lt;code&gt;Catalog&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Catalog: Generally, we recommend&#xA;users to set default values for tables through the &lt;a href=&#34;../managing-catalogs/#configure-properties&#34;&gt;Catalog properties configuration&lt;/a&gt;, such as Self-optimizing related configurations.&lt;/li&gt;&#xA;&lt;li&gt;Table: We also recommend users to&#xA;specify customized configurations when &lt;a href=&#34;../using-tables/#create-table&#34;&gt;Create Table&lt;/a&gt;, which can also be&#xA;modified through &lt;a href=&#34;../using-tables/#modify-table&#34;&gt;Alter Table&lt;/a&gt; operations.&lt;/li&gt;&#xA;&lt;li&gt;Engine: If tuning is required in the engines, then consider configuring it at the engine level, refer to&#xA;&lt;a href=&#34;../spark-configuration/&#34;&gt;Spark&lt;/a&gt; and &lt;a href=&#34;../flink-dml/&#34;&gt;Flink&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;self-optimizing-configurations&#34;&gt;Self-optimizing configurations&lt;/h2&gt;&#xA;&lt;p&gt;Self-optimizing configurations are applicable to both Iceberg Format and Mixed streaming Format.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deployment</title>
      <link>https://amoro.apache.org/docs/0.6.1/deployment/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/deployment/</guid>
      <description>&lt;h1 id=&#34;deployment&#34;&gt;Deployment&lt;/h1&gt;&#xA;&lt;p&gt;You can choose to download the stable release package from &lt;a href=&#34;../../../download/&#34;&gt;download page&lt;/a&gt;, or the source code form &lt;a href=&#34;https://github.com/apache/incubator-amoro&#34;&gt;Github&lt;/a&gt; and compile it according to the README.&lt;/p&gt;&#xA;&lt;h2 id=&#34;system-requirements&#34;&gt;System requirements&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Java 8 is required. Java 17 is required for Trino.&lt;/li&gt;&#xA;&lt;li&gt;Optional: MySQL 5.5 or higher&lt;/li&gt;&#xA;&lt;li&gt;Optional: PostgreSQL 14.x or higher&lt;/li&gt;&#xA;&lt;li&gt;Optional: ZooKeeper 3.4.x or higher&lt;/li&gt;&#xA;&lt;li&gt;Optional: Hive (2.x or 3.x)&lt;/li&gt;&#xA;&lt;li&gt;Optional: Hadoop (2.9.x or 3.x)&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;download-the-distribution&#34;&gt;Download the distribution&lt;/h2&gt;&#xA;&lt;p&gt;All released package can be downloaded from &lt;a href=&#34;../../../download/&#34;&gt;download page&lt;/a&gt;.&#xA;You can download amoro-x.y.z-bin.zip (x.y.z is the release number), and you can also download the runtime packages for each engine version according to the engine you are using.&#xA;Unzip it to create the amoro-x.y.z directory in the same directory, and then go to the amoro-x.y.z directory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deployment On Kubernetes</title>
      <link>https://amoro.apache.org/docs/0.6.1/deployment-on-kubernetes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/deployment-on-kubernetes/</guid>
      <description>&lt;h1 id=&#34;deploy-ams-on-kubernetes&#34;&gt;Deploy AMS On Kubernetes&lt;/h1&gt;&#xA;&lt;h2 id=&#34;requirements&#34;&gt;Requirements&lt;/h2&gt;&#xA;&lt;p&gt;If you want to deploy AMS on Kubernetes, you’d better get a sense of the following things.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Use AMS official docker image or build AMS docker image&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/setup/&#34;&gt;An active Kubernetes cluster&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/tools/#kubectl&#34;&gt;Kubectl&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://helm.sh/docs/intro/quickstart/&#34;&gt;Helm3+&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;amoro-official-docker-image&#34;&gt;Amoro Official Docker Image&lt;/h2&gt;&#xA;&lt;p&gt;You can find the official docker image at &lt;a href=&#34;https://hub.docker.com/u/arctic163&#34;&gt;Amoro Docker Hub&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The following are images that can be used in a production environment.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;arctic163/amoro&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;This is an image built based on the Amoro binary distribution package for deploying AMS.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink DataStream</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-datastream/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-datastream/</guid>
      <description>&lt;h1 id=&#34;flink-datastream&#34;&gt;Flink DataStream&lt;/h1&gt;&#xA;&lt;h2 id=&#34;reading-with-datastream&#34;&gt;Reading with DataStream&lt;/h2&gt;&#xA;&lt;p&gt;Amoro supports reading data in Batch or Streaming mode through Java API.&lt;/p&gt;&#xA;&lt;h3 id=&#34;batch-mode&#34;&gt;Batch mode&lt;/h3&gt;&#xA;&lt;p&gt;Using Batch mode to read the full and incremental data in the FileStore.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Non-primary key tables support reading full data in batch mode, snapshot data with a specified snapshot-id or timestamp, and incremental data with a specified snapshot interval.&lt;/li&gt;&#xA;&lt;li&gt;The primary key table temporarily only supports reading the current full amount and later CDC data.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;StreamExecutionEnvironment env &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; StreamExecutionEnvironment.&lt;span style=&#34;color:#a6e22e&#34;&gt;createLocalEnvironment&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;InternalCatalogBuilder catalogBuilder &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    InternalCatalogBuilder&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;builder&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;metastoreUrl&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;thrift://&amp;lt;url&amp;gt;:&amp;lt;port&amp;gt;/&amp;lt;catalog_name&amp;gt;&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;TableIdentifier tableId &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; TableIdentifier.&lt;span style=&#34;color:#a6e22e&#34;&gt;of&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;catalog_name&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;database_name&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;test_table&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;AmoroTableLoader tableLoader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AmoroTableLoader.&lt;span style=&#34;color:#a6e22e&#34;&gt;of&lt;/span&gt;(tableId, catalogBuilder);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Map&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;String, String&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; properties &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; HashMap&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;gt;&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;//  Default is true.&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;properties.&lt;span style=&#34;color:#a6e22e&#34;&gt;put&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;streaming&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;false&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DataStream&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;RowData&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    FlinkSource.&lt;span style=&#34;color:#a6e22e&#34;&gt;forRowData&lt;/span&gt;()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;env&lt;/span&gt;(env)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;tableLoader&lt;/span&gt;(tableLoader)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// The primary key table only supports reading the current full amount and later CDC data temporarily, without the properties parameter .&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;properties&lt;/span&gt;(properties)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        .&lt;span style=&#34;color:#a6e22e&#34;&gt;build&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// print All data read&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;batch.&lt;span style=&#34;color:#a6e22e&#34;&gt;print&lt;/span&gt;();&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// Submit and execute the task&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;env.&lt;span style=&#34;color:#a6e22e&#34;&gt;execute&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Test Amoro Batch Read&amp;#34;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The map properties contain below keys, &lt;strong&gt;currently only valid for non-primary key tables&lt;/strong&gt;:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink DDL</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-ddl/</guid>
      <description>&lt;h1 id=&#34;flink-ddl&#34;&gt;Flink DDL&lt;/h1&gt;&#xA;&lt;h2 id=&#34;create-catalogs&#34;&gt;Create catalogs&lt;/h2&gt;&#xA;&lt;h3 id=&#34;flink-sql&#34;&gt;Flink SQL&lt;/h3&gt;&#xA;&lt;p&gt;The following statement can be executed to create a Flink catalog:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;CATALOG&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;catalog_name&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;WITH&lt;/span&gt; (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;type&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;arctic&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;`&amp;lt;&lt;/span&gt;config_key&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;`=`&amp;lt;&lt;/span&gt;config_value&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;`&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;); &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Where &lt;code&gt;&amp;lt;catalog_name&amp;gt;&lt;/code&gt; is the user-defined name of the Flink catalog, and &lt;code&gt;&amp;lt;config_key&amp;gt;&lt;/code&gt;=&lt;code&gt;&amp;lt;config_value&amp;gt;&lt;/code&gt; has the following configurations:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Key&lt;/th&gt;&#xA;          &lt;th&gt;Default Value&lt;/th&gt;&#xA;          &lt;th&gt;Type&lt;/th&gt;&#xA;          &lt;th&gt;Required&lt;/th&gt;&#xA;          &lt;th&gt;Description&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;metastore.url&lt;/td&gt;&#xA;          &lt;td&gt;(none)&lt;/td&gt;&#xA;          &lt;td&gt;String&lt;/td&gt;&#xA;          &lt;td&gt;Yes&lt;/td&gt;&#xA;          &lt;td&gt;The URL for Amoro Metastore is thrift://&lt;code&gt;&amp;lt;ip&amp;gt;&lt;/code&gt;:&lt;code&gt;&amp;lt;port&amp;gt;&lt;/code&gt;/&lt;code&gt;&amp;lt;catalog_name_in_metastore&amp;gt;&lt;/code&gt;.&lt;br&gt;If high availability is enabled for AMS, it can also be specified in the form of zookeeper://{zookeeper-server}/{cluster-name}/{catalog-name}.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;default-database&lt;img width=100/&gt;&lt;/td&gt;&#xA;          &lt;td&gt;default&lt;/td&gt;&#xA;          &lt;td&gt;String&lt;/td&gt;&#xA;          &lt;td&gt;No&lt;/td&gt;&#xA;          &lt;td&gt;The default database to use&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;property-version&lt;/td&gt;&#xA;          &lt;td&gt;1&lt;/td&gt;&#xA;          &lt;td&gt;Integer&lt;/td&gt;&#xA;          &lt;td&gt;No&lt;/td&gt;&#xA;          &lt;td&gt;Catalog properties version, this option is for future backward compatibility&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;The authentication information of AMS catalog can upload configuration files on AMS website,&#xA;or specify the authentication information and configuration file paths when creating catalogs with Flink DDL&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink DML</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-dml/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-dml/</guid>
      <description>&lt;h1 id=&#34;flink-dml&#34;&gt;Flink DML&lt;/h1&gt;&#xA;&lt;h2 id=&#34;querying-with-sql&#34;&gt;Querying with SQL&lt;/h2&gt;&#xA;&lt;p&gt;Amoro tables support reading data in stream or batch mode through Flink SQL. You can switch modes using the following methods:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;-- Run Flink tasks in streaming mode in the current session&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; execution.runtime&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; streaming;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;-- Run Flink tasks in batch mode in the current session&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SET&lt;/span&gt; execution.runtime&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;mode&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batch;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;batch-mode&#34;&gt;Batch mode&lt;/h3&gt;&#xA;&lt;p&gt;Use batch mode to read full and incremental data from FileStore.&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;&lt;strong&gt;TIPS&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;LogStore does not support bounded reading.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flink Getting Started</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-getting-started/</guid>
      <description>&lt;h1 id=&#34;flink-getting-started&#34;&gt;Flink Getting Started&lt;/h1&gt;&#xA;&lt;h2 id=&#34;iceberg-format&#34;&gt;Iceberg format&lt;/h2&gt;&#xA;&lt;p&gt;The Iceberg Format can be accessed using the Connector provided by Iceberg.&#xA;Refer to the documentation at &lt;a href=&#34;https://iceberg.apache.org/docs/latest/flink-connector/&#34;&gt;Iceberg Flink user manual&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;paimon-format&#34;&gt;Paimon format&lt;/h2&gt;&#xA;&lt;p&gt;The Paimon Format can be accessed using the Connector provided by Paimon.&#xA;Refer to the documentation at &lt;a href=&#34;https://paimon.apache.org/docs/master/engines/flink/&#34;&gt;Paimon Flink user manual&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;mixed-format&#34;&gt;Mixed format&lt;/h2&gt;&#xA;&lt;p&gt;The Apache Flink engine can process Amoro table data in batch and streaming mode. The Flink on Amoro connector provides the ability to read and write to the Amoro data lake while ensuring data consistency. To meet the high real-time data requirements of businesses, the Amoro data lake&amp;rsquo;s underlying storage structure is designed with LogStore, which stores the latest changelog or append-only real-time data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Iceberg</title>
      <link>https://amoro.apache.org/docs/0.6.1/iceberg-format/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/iceberg-format/</guid>
      <description>&lt;h1 id=&#34;iceberg-format&#34;&gt;Iceberg Format&lt;/h1&gt;&#xA;&lt;p&gt;Iceberg format refers to &lt;a href=&#34;https://iceberg.apache.org&#34;&gt;Apache Iceberg&lt;/a&gt; table, which is an open table format for large analytical datasets designed to provide scalable, efficient, and secure data storage and query solutions.&#xA;It supports data operations on multiple storage backends and provides features such as ACID transactions, multi-version control, and schema evolution, making data management and querying more flexible and convenient.&lt;/p&gt;&#xA;&lt;p&gt;With the release of &lt;a href=&#34;https://iceberg.apache.org/spec/&#34;&gt;Iceberg v2&lt;/a&gt;,  Iceberg addresses the shortcomings of row-level updates through the MOR (Merge On Read) mechanism, which better supports streaming updates.&#xA;However, as data and delete files are written, the read performance and availability of the table will decrease, and if not maintained in time, the table will quickly become unusable.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Managing Catalogs</title>
      <link>https://amoro.apache.org/docs/0.6.1/managing-catalogs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/managing-catalogs/</guid>
      <description>&lt;h1 id=&#34;managing-catalogs&#34;&gt;Managing Catalogs&lt;/h1&gt;&#xA;&lt;p&gt;Users can import your test or online clusters through the catalog management function provided by the AMS Dashboard. Before adding a new Catalog,&#xA;please read the following guidelines and select the appropriate creation according to your actual needs.&lt;/p&gt;&#xA;&lt;h2 id=&#34;create-catalog&#34;&gt;Create catalog&lt;/h2&gt;&#xA;&lt;p&gt;In Amoro, the catalog is a namespace for a group of libraries and tables. Under the catalog, it is further divided into different databases, and under each database, there are different tables. The name of a table in Amoro is uniquely identified by the format &lt;code&gt;catalog.database.table&lt;/code&gt;. In practical applications, a catalog generally corresponds to a metadata service, such as the commonly used Hive Metastore in big data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Managing Optimizers</title>
      <link>https://amoro.apache.org/docs/0.6.1/managing-optimizers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/managing-optimizers/</guid>
      <description>&lt;h1 id=&#34;managing-optimizers&#34;&gt;Managing Optimizers&lt;/h1&gt;&#xA;&lt;p&gt;The optimizer is the execution unit for performing self-optimizing tasks on a table. To isolate optimizing tasks on different tables and support the deployment of optimizers in different environments, Amoro has proposed the concepts of optimizer containers and optimizer groups:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Optimizer container: Encapsulate the deployment method of optimizers, there are three implementations for now: &lt;code&gt;flink container&lt;/code&gt; based on Flink streaming job, &lt;code&gt;local container&lt;/code&gt; based on Java Application, and &lt;code&gt;external container&lt;/code&gt; based on manually started by users.&lt;/li&gt;&#xA;&lt;li&gt;Optimizer group: A collection of optimizers, where each table must select an optimizer group to perform optimizing tasks on it. Tables under the same optimizer group contribute resources to each other, and tables under different optimizer groups can be isolated in terms of optimizer resources.&lt;/li&gt;&#xA;&lt;li&gt;Optimizer: The specific unit that performs optimizing tasks, usually with multiple concurrent units.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;optimizer-container&#34;&gt;Optimizer container&lt;/h2&gt;&#xA;&lt;p&gt;Before using self-optimizing, you need to configure the container information in the configuration file. Optimizer container represents a specific set of runtime environment configuration, and the scheduling scheme of optimizer in that runtime environment. The container includes three types: flink, local, and external.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Metrics</title>
      <link>https://amoro.apache.org/docs/0.6.1/metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/metrics/</guid>
      <description>&lt;h1 id=&#34;metrics&#34;&gt;Metrics&lt;/h1&gt;&#xA;&lt;p&gt;Amoro provides both table-level and platform-level metrics to help users understand the runtime status of the current table.&lt;/p&gt;&#xA;&lt;h2 id=&#34;table-metrics&#34;&gt;Table metrics&lt;/h2&gt;&#xA;&lt;p&gt;The Amoro Tables details page provides multiple tabs to display the status of the table from various dimensions, mainly including:&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Tab Name&lt;/strong&gt;&lt;/th&gt;&#xA;          &lt;th&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Details&lt;/td&gt;&#xA;          &lt;td&gt;Display the table&amp;rsquo;s schema, primary key configuration, partition configuration, properties; as well as the metric information of the files stored in ChangeStore and BaseStore, including the number of files and average file size, as well as the latest submission time of the files.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Transactions&lt;/td&gt;&#xA;          &lt;td&gt;The displayed transaction list does not include snapshots generated by Self-optimizing.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Optimized&lt;/td&gt;&#xA;          &lt;td&gt;Display all the historical Optimize records of the table, each record shows the number and average size of files before and after Optimize, as well as the execution time of each Optimize.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;Operations&lt;/td&gt;&#xA;          &lt;td&gt;Display the current table&amp;rsquo;s DDL historical change records.&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;&lt;img src=&#34;../images/admin/table_metrics.png&#34; alt=&#34;table-details&#34;&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixed-Hive</title>
      <link>https://amoro.apache.org/docs/0.6.1/mixed-hive-format/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/mixed-hive-format/</guid>
      <description>&lt;h1 id=&#34;mixed-hive-format&#34;&gt;Mixed-Hive Format&lt;/h1&gt;&#xA;&lt;p&gt;Mixed-Hive format is a format that has better compatibility with Hive than Mixed-Iceberg format.&#xA;Mixed-Hive format uses a Hive table as the BaseStore and an Iceberg table as the ChangeStore. Mixed-Hive format supports:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;schema, partition, and types consistent with Hive format&lt;/li&gt;&#xA;&lt;li&gt;Using the Hive connector to read and write Mixed-Hive format tables as Hive tables&lt;/li&gt;&#xA;&lt;li&gt;Upgrading a Hive table in-place to a Mixed-Hive format table without data rewriting or migration, with a response time in seconds&lt;/li&gt;&#xA;&lt;li&gt;All the functional features of Mixed-Iceberg format&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The structure of Mixed-Hive format is shown below:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Mixed-Iceberg</title>
      <link>https://amoro.apache.org/docs/0.6.1/mixed-iceberg-format/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/mixed-iceberg-format/</guid>
      <description>&lt;h1 id=&#34;mixed-iceberg-format&#34;&gt;Mixed-Iceberg Format&lt;/h1&gt;&#xA;&lt;p&gt;Compared with Iceberg format, Mixed-Iceberg format provides more features:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Stronger primary key constraints that also apply to Spark&lt;/li&gt;&#xA;&lt;li&gt;OLAP performance that is production-ready for real-time data warehouses through the auto-bucket mechanism&lt;/li&gt;&#xA;&lt;li&gt;LogStore configuration that can reduce data pipeline latency from minutes to milliseconds/seconds&lt;/li&gt;&#xA;&lt;li&gt;Transaction conflict resolution mechanism that enables concurrent writes with the same primary key&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The design intention of Mixed-Iceberg format is to provide a storage layer for stream-batch integration and offline-real-time unified data warehouses for big data platforms based on data lakes.&#xA;Under this goal-driven approach, Amoro designs Mixed-Iceberg format as a three-tier structure, with each level named after a different TableStore:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Overview</title>
      <link>https://amoro.apache.org/docs/0.6.1/formats-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/formats-overview/</guid>
      <description>&lt;h1 id=&#34;formats-overview&#34;&gt;Formats Overview&lt;/h1&gt;&#xA;&lt;p&gt;Table format (aka. format) was first proposed by Iceberg, which can be described as follows:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;It defines the relationship between tables and files, and any engine can query and retrieve data files according to the table format.&lt;/li&gt;&#xA;&lt;li&gt;New formats such as Iceberg/Delta/Hudi further define the relationship between tables and snapshots, and the relationship between snapshots and files.&#xA;All write operations on the table will generate new snapshots, and all read operations on the table are based on snapshots.&#xA;Snapshots bring MVCC, ACID, and Transaction capabilities to data lakes.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;In addition, new table formats such as &lt;a href=&#34;https://Iceberg.apache.org/&#34;&gt;Iceberg&lt;/a&gt; also provide many advanced features such as schema evolve, hidden partition, and data skip.&#xA;&lt;a href=&#34;https://hudi.apache.org/&#34;&gt;Hudi&lt;/a&gt; and &lt;a href=&#34;https://delta.io/&#34;&gt;Delta&lt;/a&gt; may have some differences in specific functions, but we see that the standard of table formats is gradually established with the functional convergence of these three open-source projects in the past two years.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Self-Optimizing</title>
      <link>https://amoro.apache.org/docs/0.6.1/self-optimizing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/self-optimizing/</guid>
      <description>&lt;h1 id=&#34;self-optimizing&#34;&gt;Self-optimizing&lt;/h1&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Lakehouse is characterized by its openness and loose coupling, with data and files maintained by users through various engines. While this&#xA;architecture appears to be well-suited for T+1 scenarios, as more attention is paid to applying Lakehouse to streaming data warehouses and real-time&#xA;analysis scenarios, challenges arise. For example:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Streaming writes bring a massive amount of fragment files&lt;/li&gt;&#xA;&lt;li&gt;CDC ingestion and streaming updates generate excessive redundant data&lt;/li&gt;&#xA;&lt;li&gt;Using the new data lake format leads to orphan files and expired snapshots.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;These issues can significantly affect the performance and cost of data analysis. Therefore, Amoro has introduced a Self-optimizing mechanism to&#xA;create an out-of-the-box Streaming Lakehouse management service that is as user-friendly as a traditional database or data warehouse. The new table&#xA;format is used for this purpose. Self-optimizing involves various procedures such as file compaction, deduplication, and sorting.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Configuration</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-configuration/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-configuration/</guid>
      <description>&lt;h1 id=&#34;spark-configuration&#34;&gt;Spark Configuration&lt;/h1&gt;&#xA;&lt;h2 id=&#34;catalogs-configuration&#34;&gt;Catalogs configuration&lt;/h2&gt;&#xA;&lt;h3 id=&#34;using-mixed-format-in-a-standalone-catalog&#34;&gt;Using Mixed-Format in a standalone catalog&lt;/h3&gt;&#xA;&lt;p&gt;Starting from version 3.x, Spark supports configuring an independent Catalog.&#xA;If you want to use a Mixed-Format table in a standalone Catalog, you can configure it as follows:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-properties&#34; data-lang=&#34;properties&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;spark.sql.catalog.arctic_catalog&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;com.netease.arctic.spark.ArcticSparkCatalog&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;spark.sql.catalog.arctic_catalog.url&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;thrift://${AMS_HOST}:${AMS_PORT}/${AMS_CATALOG_NAME_HIVE}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then, execute the following SQL in the Spark SQL Client to switch to the corresponding catalog.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;use arctic_catalog;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Of course, you can also access Mixed-Format tables by directly using the triplet&#xA;&lt;code&gt;arctic_catalog.{db_name}.{table_name}&lt;/code&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark DDL</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-ddl/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-ddl/</guid>
      <description>&lt;h1 id=&#34;spark-ddl&#34;&gt;Spark DDL&lt;/h1&gt;&#xA;&lt;h2 id=&#34;create-table&#34;&gt;CREATE TABLE&lt;/h2&gt;&#xA;&lt;p&gt;To create an MixedFormat table under an Amoro Catalog, you can use &lt;code&gt;USING ARCTIC&lt;/code&gt; to specify the provider in the&#xA;&lt;code&gt;CREATE TABLE&lt;/code&gt; statement. If the Catalog type is Hive, the created table will be a Hive-compatible table.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; arctic_catalog.db.sample (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    id bigint  &lt;span style=&#34;color:#66d9ef&#34;&gt;COMMENT&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;unique id&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;data&lt;/span&gt; string&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;USING&lt;/span&gt; arctic &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;primary-key&#34;&gt;PRIMARY KEY&lt;/h3&gt;&#xA;&lt;p&gt;You can use &lt;code&gt;PRIMARY KEY&lt;/code&gt; in the &lt;code&gt;CREATE TABLE&lt;/code&gt; statement to specify the primary key column.&#xA;MixedFormat ensures the uniqueness of the primary key column through MOR (Merge on Read) and Self-Optimizing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Getting Started</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-getting-started/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-getting-started/</guid>
      <description>&lt;h1 id=&#34;spark-getting-started&#34;&gt;Spark Getting Started&lt;/h1&gt;&#xA;&lt;h1 id=&#34;iceberg-format&#34;&gt;Iceberg Format&lt;/h1&gt;&#xA;&lt;p&gt;The Iceberg Format can be accessed using the Connector provided by Iceberg.&#xA;Refer to the documentation at &lt;a href=&#34;https://iceberg.apache.org/docs/latest/getting-started/&#34;&gt;Iceberg Spark Connector&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h1 id=&#34;paimon-format&#34;&gt;Paimon Format&lt;/h1&gt;&#xA;&lt;p&gt;The Paimon Format can be accessed using the Connector provided by Paimon.&#xA;Refer to the documentation at &lt;a href=&#34;https://paimon.apache.org/docs/master/engines/spark3/&#34;&gt;Paimon Spark Connector&lt;/a&gt;&#xA;for more information.&lt;/p&gt;&#xA;&lt;h1 id=&#34;mixed-format&#34;&gt;Mixed Format&lt;/h1&gt;&#xA;&lt;p&gt;To use Amoro in a Spark shell, use the &amp;ndash;packages option:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;spark-shell --packages com.netease.amoro:amoro-spark-3.3-runtime:0.5.0&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;blockquote&gt;&#xA;&lt;p&gt;If you want to include the connector in your Spark installation, add the &lt;code&gt;amoro-spark-3.3-runtime&lt;/code&gt; Jar to&#xA;Spark&amp;rsquo;s &lt;code&gt;jars&lt;/code&gt; folder.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Queries</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-queries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-queries/</guid>
      <description>&lt;h1 id=&#34;spark-queries&#34;&gt;Spark Queries&lt;/h1&gt;&#xA;&lt;h2 id=&#34;querying-with-sql&#34;&gt;Querying with SQL&lt;/h2&gt;&#xA;&lt;h3 id=&#34;querying-mixed-format-table-by-merge-on-read&#34;&gt;Querying Mixed-Format table by merge on read&lt;/h3&gt;&#xA;&lt;p&gt;Using &lt;code&gt;Select&lt;/code&gt; statement to query on Mixed-Format tables.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; arctic_catalog.db.sample&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The Mixed-Format connector will merge the data from &lt;code&gt;BaseStore&lt;/code&gt; and &lt;code&gt;ChangeStore&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;h3 id=&#34;query-on-change-store&#34;&gt;Query on change store&lt;/h3&gt;&#xA;&lt;p&gt;For a Mixed-Format table with primary keys. you can query on &lt;code&gt;ChangeStore&lt;/code&gt; by &lt;code&gt;.change&lt;/code&gt;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;SELECT&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt; arctic_catalog.db.sample.change&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;---+----+----+---------------+------------+--------------+&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt; id&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;name&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;data&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_transaction_id&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_file_offset&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;_change_action&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;---+----+----+---------------+------------+--------------+&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;dddd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;abcd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;              &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;INSERT&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;  &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;dddd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;abcd&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;              &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;           &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;DELETE&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;|&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;---+----+----+---------------+------------+--------------+&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The addition columns are:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;_transaction_id: The transaction ID allocated by AMS during data write is assigned per SQL execution in batch mode and&#xA;per checkpoint in streaming mode.&lt;/li&gt;&#xA;&lt;li&gt;_file_offset：The order of data written with the same &lt;code&gt;_transaction_id&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;li&gt;_change_action：The type of change record, &lt;code&gt;INSERT&lt;/code&gt; or &lt;code&gt;DELETE&lt;/code&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;querying-with-dataframes&#34;&gt;Querying with DataFrames&lt;/h2&gt;&#xA;&lt;p&gt;You can read the Mixed-Format table by Spark DataFrames:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Writes</title>
      <link>https://amoro.apache.org/docs/0.6.1/spark-writes/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/spark-writes/</guid>
      <description>&lt;h1 id=&#34;spark-writes&#34;&gt;Spark Writes&lt;/h1&gt;&#xA;&lt;h2 id=&#34;writing-with-sql&#34;&gt;Writing with SQL&lt;/h2&gt;&#xA;&lt;h3 id=&#34;insert-overwrite&#34;&gt;INSERT OVERWRITE&lt;/h3&gt;&#xA;&lt;p&gt;&lt;code&gt;INSERT OVERWRITE&lt;/code&gt; can replace the partition in a table with the results of a query.&lt;/p&gt;&#xA;&lt;p&gt;The default overwrite mode of Spark is &lt;code&gt;Static&lt;/code&gt;, you can change the overwrite mode by&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;SET spark.sql.sources.partitionOverwriteMode=dynamic&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To demonstrate the behavior of dynamic and static overwrites, a test table is defined using the following DDL:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CREATE&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;TABLE&lt;/span&gt; arctic_catalog.db.sample (&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    id int,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;data&lt;/span&gt; string,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ts &lt;span style=&#34;color:#66d9ef&#34;&gt;timestamp&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;primary&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;key&lt;/span&gt; (id))&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;USING&lt;/span&gt; arctic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;PARTITIONED &lt;span style=&#34;color:#66d9ef&#34;&gt;BY&lt;/span&gt; (days(ts))&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When Spark&amp;rsquo;s overwrite mode is dynamic, the partitions of the rows generated by the SELECT query will be replaced.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Table Watermark</title>
      <link>https://amoro.apache.org/docs/0.6.1/table-watermark/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/table-watermark/</guid>
      <description>&lt;h1 id=&#34;table-watermark&#34;&gt;Table Watermark&lt;/h1&gt;&#xA;&lt;h2 id=&#34;table-freshness&#34;&gt;Table freshness&lt;/h2&gt;&#xA;&lt;p&gt;Data freshness represents timeliness, and in many discussions, freshness is considered one of the important indicators of data quality. In traditional&#xA;offline data warehouses, higher cost typically means better performance, creating a typical binary paradox in terms of cost-performance trade-off.&#xA;However, in high-freshness streaming data warehouses, massive small files and frequent updates can lead to performance degradation. The higher the&#xA;freshness, the greater the impact on performance. To achieve the required performance, users must incur higher costs. Thus, for streaming data&#xA;warehouses, data freshness, query performance, and cost form a tripartite paradox.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Trino</title>
      <link>https://amoro.apache.org/docs/0.6.1/trino/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/trino/</guid>
      <description>&lt;h1 id=&#34;trino&#34;&gt;Trino&lt;/h1&gt;&#xA;&lt;h2 id=&#34;iceberg-format&#34;&gt;Iceberg format&lt;/h2&gt;&#xA;&lt;p&gt;Iceberg format can be accessed using the Iceberg Connector provided by Trino.&#xA;please refer to the documentation at &lt;a href=&#34;https://trino.io/docs/current/connector/iceberg.html#&#34;&gt;Iceberg Trino user manual&lt;/a&gt; for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;paimon-format&#34;&gt;Paimon format&lt;/h2&gt;&#xA;&lt;p&gt;Paimon format can be accessed using the Paimon Connector provided by Trino.&#xA;please refer to the documentation at &lt;a href=&#34;https://paimon.apache.org/docs/master/engines/trino/&#34;&gt;Paimon Trino user manual&lt;/a&gt; for more information.&lt;/p&gt;&#xA;&lt;h2 id=&#34;mixed-format&#34;&gt;Mixed format&lt;/h2&gt;&#xA;&lt;h3 id=&#34;install&#34;&gt;Install&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Create the {trino_home}/plugin/amoro directory in the Trino installation package,&#xA;and extract the contents of the amoro-trino package trino-amoro-xx-SNAPSHOT.tar.gz to the {trino_home}/plugin/amoro directory.&lt;/li&gt;&#xA;&lt;li&gt;Configure the Catalog configuration file for Amoro in the {trino_home}/etc/catalog directory, for example:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-tex&#34; data-lang=&#34;tex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;connector.name=arctic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;arctic.url=thrift://{ip}:{port}/{catalogName}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;Configure the JVM configuration file for Trino in the {trino_home}/etc directory named &lt;code&gt;jvm.config&lt;/code&gt; :&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-tex&#34; data-lang=&#34;tex&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;--add-exports=java.security.jgss/sun.security.krb5=ALL-UNNAMED&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;support-sql-statement&#34;&gt;Support SQL statement&lt;/h3&gt;&#xA;&lt;h4 id=&#34;query-table&#34;&gt;Query Table&lt;/h4&gt;&#xA;&lt;p&gt;By adopting the Merge-On-Read approach to read Mixed Format, the latest data of the table can be read, for example:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Kyuubi By Terminal</title>
      <link>https://amoro.apache.org/docs/0.6.1/using-kyuubi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/using-kyuubi/</guid>
      <description>&lt;h1 id=&#34;using-kyuubi-by-terminal&#34;&gt;Using Kyuubi By Terminal&lt;/h1&gt;&#xA;&lt;p&gt;&lt;strong&gt;Prerequisites&lt;/strong&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;There must be a running Kyuubi. To deploy and run Kyuubi, please refer to &lt;a href=&#34;https://kyuubi.readthedocs.io/en/master/&#34;&gt;Kyuubi doc&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Terminal supports interfacing with Kyuubi to submit SQL to Kyuubi for execution. All you need to do is add the Kyuubi configuration as instructed below:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ams:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    terminal:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      backend: kyuubi&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      kyuubi.jdbc.url: jdbc:hive2://127.0.0.1:10009/ &lt;span style=&#34;color:#75715e&#34;&gt;# kyuubi Connection Address&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Without configuring Kyuubi, Terminal executes in memory in AMS.&lt;/p&gt;&#xA;&lt;p&gt;To execute SQL in Terminal, you can refer to the following steps:：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Logstore</title>
      <link>https://amoro.apache.org/docs/0.6.1/flink-using-logstore/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/flink-using-logstore/</guid>
      <description>&lt;h1 id=&#34;using-logstore&#34;&gt;Using Logstore&lt;/h1&gt;&#xA;&lt;p&gt;Due to the limitations of traditional offline data warehouse architectures in supporting real-time business needs, real-time data warehousing has experienced rapid evolution in recent years. In the architecture of real-time data warehousing, Apache Kafka is often used as the storage system for real-time data. However, this also brings about the issue of data disconnection between offline data warehouses.&lt;/p&gt;&#xA;&lt;p&gt;Developers often need to pay attention to data stored in HDFS as well as data in Kafka, which increases the complexity of business development. Therefore, Amoro proposes the addition of an optional parameter, &amp;ldquo;LogStore enabled&amp;rdquo; (&lt;code&gt;log-store.enabled&lt;/code&gt;), to the table configuration. This allows for retrieving data with sub-second and minute-level latency by operating on a single table while ensuring the eventual consistency of data from both sources.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Using Tables</title>
      <link>https://amoro.apache.org/docs/0.6.1/using-tables/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://amoro.apache.org/docs/0.6.1/using-tables/</guid>
      <description>&lt;h1 id=&#34;using-tables&#34;&gt;Using Tables&lt;/h1&gt;&#xA;&lt;p&gt;The SQL execution tool &lt;code&gt;Terminal&lt;/code&gt; is provided in AMS dashboard to help users quickly create, modify and delete tables.&#xA;It is also available in &lt;a href=&#34;../spark-ddl/&#34;&gt;Spark&lt;/a&gt; and &lt;a href=&#34;../flink-ddl/&#34;&gt;Flink&lt;/a&gt; and other engines to manage tables using SQL.&lt;/p&gt;&#xA;&lt;h2 id=&#34;create-table&#34;&gt;Create table&lt;/h2&gt;&#xA;&lt;p&gt;After logging into AMS dashboard, go to &lt;code&gt;Terminal&lt;/code&gt;, enter the table creation statement and execute it to complete the table creation.&#xA;The following is an example of table creation:&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;create&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;table&lt;/span&gt; test_db.test_log_store(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  id int,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  name string,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  op_time &lt;span style=&#34;color:#66d9ef&#34;&gt;timestamp&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;primary&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;key&lt;/span&gt;(id)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; arctic&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;partitioned &lt;span style=&#34;color:#66d9ef&#34;&gt;by&lt;/span&gt;(days(op_time))&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tblproperties(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;log-store.enable&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;true&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;log-store.type&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;kafka&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;log-store.address&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;127.0.0.1:9092&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;log-store.topic&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;local_catalog.test_db.test_log_store.log_store&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;table.event-time-field&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;op_time&amp;#39;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;table.watermark-allowed-lateness-second&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;60&amp;#39;&lt;/span&gt;);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Currently, terminal uses Spark Engine for SQL execution. For more information on the syntax of creating tables, refer to &lt;a href=&#34;../spark-ddl/#create-table&#34;&gt;Spark DDL&lt;/a&gt;. Different Catalogs create different table formats, refer to &lt;a href=&#34;../managing-catalogs/#create-catalog&#34;&gt;Create Catalog&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
